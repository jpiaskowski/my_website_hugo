<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data workflow | Julia Piaskowski</title>
    <link>/tags/data-workflow/</link>
      <atom:link href="/tags/data-workflow/index.xml" rel="self" type="application/rss+xml" />
    <description>data workflow</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Julia Piaskowski, 2020</copyright><lastBuildDate>Sat, 01 Feb 2020 10:50:02 -0700</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>data workflow</title>
      <link>/tags/data-workflow/</link>
    </image>
    
    <item>
      <title>Bad Data Management: A Star Wars Story</title>
      <link>/post/starwars_bad_data/</link>
      <pubDate>Sat, 01 Feb 2020 10:50:02 -0700</pubDate>
      <guid>/post/starwars_bad_data/</guid>
      <description>&lt;p&gt;Most of us think of Star Wars as the rebellion versus the empire &amp;ndash; good versus evil with key figures: the Jedi, the Sith, Darth Vader, Luke Skywalker, the Emperor, Rey, Kylo Ren. But, The downfall of the governing bodies was not Darth Vader, Luke Skywalker, Kylo Ren or Palpatine. Actually, Star Wars is a story of really bad data management practices.&lt;/p&gt;
&lt;h4 id=&#34;the-republic&#34;&gt;The Republic:&lt;/h4&gt;
&lt;p&gt;The Jedi maintained decent, well-organised archives in a large library and they even had paid librarians on staff. But any jedi can delete information and no record is left regarding this? For heaven’s sake, an entire planet was removed from said archives That ain’t good. It led to the Clone wars and eventual rise of the Galactic Empire. It also turns out that a person just has to look like a jedi in order 
&lt;a href=&#34;https://starwars.fandom.com/wiki/Holocron_Heist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;to steal secured jedi archives&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;younglings.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;original-galatic-empire&#34;&gt;Original Galatic Empire:*&lt;/h4&gt;
&lt;p&gt;This is where all the bad stuff starts. Initially, they had a good system for managing their vasts amount of data and an off-site backup with a very powerful firewall. However, once that firewall was breached, there was poor security inside - I mean, a dead archivist’s hand could be used to access &lt;em&gt;the entire system&lt;/em&gt;, presumably as a root user. Their storage facility also relied on a rather inconvenient system for accessing data: a massive tower than required a remote-controlled tool for physical retrieval of data files.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;archive_joystick.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool itself required manual operation and apparently the entire facility had no backup generator to maintain secruity and functioning in case of a power loss. Incredibly, there was also no search tool. Data archives had to be &lt;em&gt;browsed&lt;/em&gt; (what???). it’s really quite astonishing that anyone ever found what they needed. Options for sending out the data were also remarkably cumbersome - a satellite dish the size of a Star Destroyer has to manually turned in the correct direction to send a file out (was the Empire capable of automating anything?). I also wonder about the file compression capabilities - seriously, is there any file so large it needs a satellite dish of that size?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;scarif_tower.JPG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;the-first-order&#34;&gt;The First Order:&lt;/h4&gt;
&lt;p&gt;The First Order inherits a number of problems from the original Galatic Empire, most notably insufficient backups resulting in lost information. A map to Luke Skywalker is lost, making it nigh impossible to find him. A map to Exogol had two copies ever made, one of them on a destroyed death star sitting in the middle of raging ocean? It’s amazing these were very found. Without these maps, we have no Sith and no Jedi (maybe not such a bad thing).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;skywalker_map.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The point is that if the Jedi had better data practices, then they might have avoided the rise of the empire. And perhaps if the Empire had better data practices, they may not undergone such a rapid downfall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;death_star_wreckage.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;what-happens-with-good-data-management&#34;&gt;What happens with good data management:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Secure data that can only be modified by authorised users&lt;/li&gt;
&lt;li&gt;Validation of data values to prevent errors&lt;/li&gt;
&lt;li&gt;Data is accessible: it can be accessed in many places, by many, in formats that are easy to use&lt;/li&gt;
&lt;li&gt;Stored data is easy to search and summarise&lt;/li&gt;
&lt;li&gt;There are sufficient backups if something goes wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;wait-this-actually-matters&#34;&gt;Wait! This actually matters!&lt;/h4&gt;
&lt;p&gt;This is more than a story to amuse folks, though. Poor data management is a rampant issue across academia. I have observed that most researchers do not document their data processing in sufficient detail to recreate the process. Who else has read this statement &amp;ldquo;outliers were removed&amp;rdquo; with no details into how those outliers were eliminated or the numerical values of those &amp;ldquo;outliers&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;From a research standpoint this means the analytical procedure is not replicable. That means someone starting with the same raw data will not obtain the same results following the described analtical procedure &amp;ndash; because the description was inadequate. This can also mean data are unreliable. If the orginal version of data become lost as individuals clean up the data and do not document their process, then there is no way to evaluate if the cleaning was correctly done or justified. Data loss is always a risk when files are emailed around rather than shared via a more permanent data storage solution. All of these risks become amplified when an individual leaves a research program and can no longer be reached to resolve data questions.&lt;/p&gt;
&lt;h4 id=&#34;tools-for-better-data-management&#34;&gt;Tools for better data management&lt;/h4&gt;
&lt;p&gt;Handily, many many tools exist to assist in data management. I give a bare bones outline in this 
&lt;a href=&#34;/post/workflow_basics/&#34;&gt;post&lt;/a&gt;
 (please read this!!). Below is an even more bare &amp;lsquo;bare bones&amp;rsquo; outline.&lt;/p&gt;
&lt;p&gt;One of the best solutions is to put your data in a database. This a great tool for large quantities of data that follow a regular format and are frequently updated - such as variety testing data.&lt;/p&gt;
&lt;p&gt;However, most agricultural researchers are not so lucky to have such structured data. At the very least, maintaining research outputs in a shared and secured location is a good start. Archiving raw versions of data is paramount. This maybe never be needed, but if it is needed, there is not replacement if those data are lost. Requiring complete documentation from all research contributors on the data set generation and processing is also good. A &amp;ldquo;codebook&amp;rdquo; is another great tool. This is a file accompanying a data set that provides the name of each column, what data are present in that column, and optionally, the units of those data and the range of values expected. If a column contains contains numeric values btweeen 0 and 100, then others will know that 980 is probably an error.&lt;/p&gt;
&lt;p&gt;Most of the researchers I work with were trained in a different era - when data was expensive to obtain and as a result, there was considerably less data to deal with. I count myself among this group. At that time, data management tools were less relevant because we had the time to comprehensively examine a data set for errors. Furthermore, data sharing and data reuse were not widely done. But, we all now those days are long over. We are now expected to process a large quantity of data rapidly, analyse it and eventually arhive it publicaly for possible reuse by someone else. Engaging in practices that enable quality data management is essential for participation in today&amp;rsquo;s research environment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Practices for Data Workflows in Ag Science</title>
      <link>/post/workflow_basics/</link>
      <pubDate>Tue, 07 Aug 2018 10:50:02 -0700</pubDate>
      <guid>/post/workflow_basics/</guid>
      <description>&lt;p&gt;I was struggling with the data set. The first 50 rows were summary stats (&amp;hellip;I think), and then the actual data started. The file was over 250 columns wide, composed largely of phenotypic traits. Perhaps one third of the columns were unique variables gathered over several years, but, the variable names were inconsistent across years. Actually, there was no information on the variables, what they were measuring, and what scale they were on. The closest hint was a pattern of color coding applied to groups of columns: yellow, green, blue. There appeared to be partial duplication and/or combination of of some columns, but we could not reconstruct exactly what had happened to the data. No one could recall the full history of the file, what that color coding meant, who had pre-processed data, or what they had done. We did know that the raw data were long lost.&lt;/p&gt;
&lt;p&gt;This was the not the first nor the last messy data set with an unknown history that complicated my capacity to organize and analyse the information.&lt;/p&gt;
&lt;p&gt;Other problems I encountered with poorly managed data sets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date sets with outliers removed based on individual&amp;rsquo;s researchers assessment on what looked weird or not.&lt;/li&gt;
&lt;li&gt;An Excel file several thousand lines with miscellaneous comments added every so often indicating a data point may be suspect.&lt;/li&gt;
&lt;li&gt;An Excel file several sheets wide of the same information across different years - yet the column names did not match, and some sheets included summary stats for groups of data, using empty rows to differentiate groups (again, I think this was what going on&amp;hellip;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There&amp;rsquo;s no way to sugarcoat it: management of raw experimental data and the analysis workflow is largely abysmal in the agricultural sciences, my academic home for 14 years. The process of cleaning, rearranging, filtering and preparing experimental data for downstream analyses is poorly described, if at all, in most papers. As many of know, much of data conditioning and preparation is done by students and technicians with little formal guidance or documentation of their work.&lt;/p&gt;
&lt;p&gt;Since data sets can and should live on for decades - as part of long-term agriculture research, for meta-analyses or other uses, understanding how the raw data were parsed, cleaned and interpreted is important. Assumptions that made sense in the initial analysis may need re-evaluation later. Intermediate files can turn out to be useful. Organization of an analysis workflow can help researchers conduct their research in a regimented and reproducible way.&lt;/p&gt;
&lt;p&gt;Here are a few concrete recommendations for improving workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Archive the raw, unaltered version of data set.&lt;/li&gt;
&lt;li&gt;Create a consistent directory structure for each project - that way you are storing things consistently and can find them. Really, this makes things easier for everyone.&lt;/li&gt;
&lt;li&gt;For a data set, each column should be include only type of data corresponding to the column name. These data can be on different scales. For example, if one column indicates the type of variable (e.g. pH, moisture content, nitrate concentration), another columns could indicate the actual value for that trait and observation. It is less confusing if a column only contain a similar type of data (e.g &lt;em&gt;raw&lt;/em&gt; OR &lt;em&gt;transformed&lt;/em&gt; OR &lt;em&gt;mean value&lt;/em&gt;), not data and summary stats together.&lt;/li&gt;
&lt;li&gt;Avoid use of formatting that cannot be interpreted as plain text. By this, I mean do not insert comments in Microsoft office files, or use text formatting tools (bolding, color coding) to differentiate observations. This information can easily become lost to time and does not translate well to other software, especially command-line software. Comments, categorical variables, et cetera should each have their own column.&lt;/li&gt;
&lt;li&gt;Summary stats of a data set belong in their own table or file.&lt;/li&gt;
&lt;li&gt;Use a non-proprietary format like CSV or TXT.&lt;/li&gt;
&lt;li&gt;Avoid creating data that relies on a particular order of the observations (for instance, every 5th row is the mean of the previous 4 rows). One sorting step can ruin this. If a special order is needed, create a column which implements this.&lt;/li&gt;
&lt;li&gt;Make a &amp;ldquo;codebook&amp;rdquo; documenting what each variable means, what scale it is on, expected levels or range of data. This can be one sheet of information - very brief, but handy. Giving variables short, descriptive names is also helpful.&lt;/li&gt;
&lt;li&gt;Document what happens to a data set - and make that information available to others. I prefer to use a command line tool like R, but a text file could do. This is to ensure your work can be duplicated. I realize programming is not for everyone, but using a command-line program like R, Python, Julia, SAS, or even bash, is one of the best ways to document how information is pre-processed. Changes made through point-and-click interfaces like Excel are not reproducible because they are almost always incompletely documented or not documented at all. In addition, manual manipulation in a spreadsheet program also results in user-generated errors: misspellings, misclassifications, cut-and-paste errors and so on.&lt;/li&gt;
&lt;li&gt;Accountability and follow up: attach people&amp;rsquo;s names to their data and/or analysis.&lt;/li&gt;
&lt;li&gt;Back up your data: across at least 2 locations over the life of your project. Version control make it possible for your work to be undone by accessing earlier versions. Git is one such tool, but saving intermediate file versions (often!) can also help. By providing descriptive comments at each new commit (a version), that can help downstream researchers sift through previous versions of the data. Also, some cloud storage providers (like Dropbox) offer options for data recovery.&lt;/li&gt;
&lt;li&gt;Coordinate with project collaborators regarding workflow expectations and processes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The core reason I am interested in these issues is to improve reproducibility. As I dealt with the aforementioned issues with data, I could not help but wonder if I was making the right choices with the data, and wish that I had more information on the background of these legacy data sets. This is particularly important as meta-studies continue to gain in popularity. Unifying data sets and leveraging their combined power is one of the priorities for agricultural productivity identified by the National Academy of Sciences in their recent report, 
&lt;a href=&#34;https://www.nap.edu/resource/25059/ScienceBreakthroughs2030ReportBrief.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Science Breakthroughs to Advance Food and Agricultural Research by 2030&lt;/em&gt;&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;For more information, see this 
&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;excellent article&lt;/em&gt;&lt;/a&gt;
 by Kara Woo and Karl Broman - it&amp;rsquo;s short and gives easy-to-follow best practices when working with spreadsheets.&lt;/p&gt;
&lt;p&gt;Scott Long at the University of Indiana put together an 
&lt;a href=&#34;https://www.ihrp.uic.edu/files/Workflow%20Slides%20JSLong%20110410.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;informative set of slides&lt;/a&gt;
 delving further into the principles of data workflow. Patrick Schloss, of the bioinformatics program &lt;em&gt;mothur&lt;/em&gt;, recently published an 
&lt;a href=&#34;https://mbio.asm.org/content/9/3/e00525-18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt;
 delving into the problem of reproducibility in microbiome work and how to fix it. Among many things, he recommended implementing robust workflow practices.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What I learned at the UseR! conference</title>
      <link>/post/user_2018/</link>
      <pubDate>Wed, 18 Jul 2018 10:50:02 -0700</pubDate>
      <guid>/post/user_2018/</guid>
      <description>&lt;h4 id=&#34;top-lessons-from-r-users-conference-held-july-10---13-in-brisbane-aus&#34;&gt;Top Lessons from R users conference held July 10 - 13 in Brisbane, AUS&lt;/h4&gt;
&lt;p&gt;With 900 registrants and dozens of talks, there is much to report (
&lt;a href=&#34;https://www.youtube.com/channel/UC_R5smHVXRYGhZYDJsnXTwg/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;videos of most talks&lt;/a&gt;
 provided by R Consortium YouTube channel) I&amp;rsquo;ll skip loads of it and just focus on the top 10 cool stuff.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The hex wall was just straight up cool. Here&amp;rsquo;s the 
&lt;a href=&#34;https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code for that&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I participated in a half day workshop on Rcpp (
&lt;a href=&#34;https://www.youtube.com/watch?v=FZ0LcJbxPF0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 and 
&lt;a href=&#34;https://www.youtube.com/watch?v=EXGhR-kyjRg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
). This continues to be a very popular suite of packages to help users increase the speed and efficiency of their programs. I say &amp;ldquo;suite&amp;rdquo; because in addition to 3 major functions provided by Rcpp, a number of accessory packages have been written to extend its functionality. Several of these accessory packages essentially are providing templates for invoking C++ commands without having to actually know C. Dirk Eddelbuttel introduced the 3 major functions for extending R with C/C++:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;evalCpp: for evaluating ad hoc anonymous expressions&lt;/li&gt;
&lt;li&gt;cppFunction: standard function written in C/C++ and invoked in a program&lt;/li&gt;
&lt;li&gt;sourceCpp: source C/C++ objects &amp;amp; functions from external file
These functions must be compiled each time a fresh R session is started. Building an Rcpp package avoids this and is easy to do:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Rcpp.package.skeleton(&amp;quot;mypackage&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;The other useful workshop I participated in was methods for speeding up R, delivered by Thomas Lumley, formerly of UW and now of U of Auckland. He focused on a few major recommendations:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vectorise whenever possible. This also implies that you know your vectorised functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lapply/apply/sapply/vapply/tapply&lt;/li&gt;
&lt;li&gt;rowMeans, rowSums&lt;/li&gt;
&lt;li&gt;colMeans, colSums&lt;/li&gt;
&lt;li&gt;Consider parallelised functions: mclapply&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Base functions are often designed to handle wildly different input. It may make sense in some instances to rewrite functions, making assumptions (e.g. about the input data) that fit your circumstances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data frames should be avoided if possible - they are expensive to create and often copied in whole when modified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using an optimized matrix algebra library (not LAPACK) may be worth the time to install that. These libraries have optimized how data is accessed from disk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some rec&amp;rsquo;s on how to handle large objects that reasonably don&amp;rsquo;t fit in memory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;buy a computer with more memory (not always a reasonable option)&lt;/li&gt;
&lt;li&gt;put your data in database and access with tools like dbplyr. One recommendation is MonetDBLite, a column-optimized database good for scientific applications for that reason.&lt;/li&gt;
&lt;li&gt;Use special file formats for bigger data sets: HDF5, GitLFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But, first and foremost, he emphasized the use of profiling tools and benchmarking functions. find out where the bottleneck in your program actually are! And be careful to not waste too much time on optimizing code - time saved may not be worth time invested. R is automatically running a JIT (just in time) compiler that makes code run faster each time it is called. It makes sense to turn this off while doing benchmarking:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;enableJIT(0)

# to turn back on:

enableJIT(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Course notes available on his github repo 
&lt;a href=&#34;https://github.com/tslumley/useRfasteR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UseRfasteR&lt;/a&gt;
.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent keynotes all-around. 
&lt;a href=&#34;https://www.youtube.com/watch?v=27FxbDtCFoc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steph da Silva&lt;/a&gt;
 emphasized the importance of &lt;em&gt;community&lt;/em&gt; in open-source community - what that can look like, how to contribute  how your contribution helps develop camaraderie to support sharing of code and development of analysts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=5033jBHFiHE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Peng discussed development of R&lt;/a&gt;
 as it strives to fit both programmers and regular ole scientists analyzing their data. He summarized his own talk quite well in this essay. He described the rise of &amp;ldquo;worse is better&amp;rdquo;, that is, the simplying of options in R to make it easier for new users to learn. The tidyverse is considered such an example. However, as a long-time user, I find dplyr&amp;rsquo;s group-and-summarise options (among others) to be incredibly handy. And I will never go back to using reshape() now that we have gather() and spread(). Even reshape2() is a massive improvement in usability!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jenny Bryan&amp;rsquo;s talk on 
&lt;a href=&#34;https://youtu.be/7oyiPBjLAWY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Code Smells and Feels&amp;rdquo;&lt;/a&gt;
 provided a great introduction to how to identify and fix poorly written code. According to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Code_smell&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
, &amp;ldquo;Code smells are usually not bugs; they are not technically incorrect and do not prevent the program from functioning. Instead, they indicate weaknesses in design that may slow down development or increase the risk of bugs or failures in the future.&amp;rdquo;
Based on the book, &lt;em&gt;Refactoring: Improving the Design of Existing Code&lt;/em&gt;, she gave a few simple directives to create simple code that is easy to understand, debug, and maintain. Hooray!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Hoist entry or exit conditionals to the top of functions&lt;/li&gt;
&lt;li&gt;Use functions as much as possible&lt;/li&gt;
&lt;li&gt;Avoid overly nested code (e.g. a long cascade of nested &amp;ldquo;if&amp;rdquo; statements)&lt;/li&gt;
&lt;li&gt;Not every &amp;ldquo;if&amp;rdquo; needs an else&lt;/li&gt;
&lt;li&gt;Consider implementing true object-oriented programming when working with classes&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t&amp;rsquo; be afraid to write short helper functions
There was also a brief discussion on how to tell a person their code smells bad (&amp;ldquo;that&amp;rsquo;s what we have funny names like &amp;lsquo;excessive use of literals&amp;rsquo;&amp;quot;).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent set of talks on data handling and workflow. In particular, 
&lt;a href=&#34;https://www.youtube.com/watch?v=GrqM2VqIQ20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a talk on the website builder&lt;/a&gt;
 for documenting data analysis called 
&lt;a href=&#34;https://github.com/jdblischak/workflowr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;workflowr&amp;rdquo;&lt;/a&gt;
. 
&lt;a href=&#34;https://www.youtube.com/watch?v=IYfZ6kd7aT0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discussion on consistent workflows&lt;/a&gt;
, recommended conventions for naming variables, creating consistent set of directories for each project, using version controls. Obvious to a data scientist, but rarely used by regular scientist. This resonates strongly with me due to ongoing issues with data management in the agricultural sciences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=3Bu7QUxzIbA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Excellent talk on precision in R&lt;/a&gt;
. Use .Machine to find out more, but basically R is only good to 16 decimal points. Subtraction of one number similar to it can result in cancellation where the result essentially slides to zero, losing precision. Ways to avoid this include working in log space, perhaps adding a zero if needed: log(x + 1), -log(1 - exp(x)). Also, there is the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/Rmpfr/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmpfr package&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=XQmBcpQl8K8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glue package&lt;/a&gt;
 - for &amp;ldquo;gluing&amp;rdquo; strings to data, like string interpolation in bash (&amp;ldquo;$&amp;rdquo;) or python f&amp;rdquo;{&amp;hellip;}&amp;quot;. R does have sprintf() with with identical functionality to C&amp;rsquo;s printf(), but glue makes this easier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=L6FawdEA3W0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fun talk on vwline package&lt;/a&gt;
 for creating variable-width lines, like in Menaurd&amp;rsquo;s famous plot of Napolean&amp;rsquo;s march through Russia.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;./napoleon.jpg&#34; alt=&#34;napolean_plot&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
