<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julia Piaskowski</title>
    <link>/authors/admin/</link>
      <atom:link href="/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Julia Piaskowski</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Julia Piaskowski, 2020</copyright><lastBuildDate>Sat, 25 Apr 2020 10:50:02 -0700</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Julia Piaskowski</title>
      <link>/authors/admin/</link>
    </image>
    
    <item>
      <title>Adventures in Babysitting: Web Scraping for the Python and HTML Novice</title>
      <link>/post/6_web_scraping_intro/</link>
      <pubDate>Sat, 25 Apr 2020 10:50:02 -0700</pubDate>
      <guid>/post/6_web_scraping_intro/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This is adapted from a talk I gave at Pycascades in Portland, Oregon 2020-02-09.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All the resources from this talk are available at my &lt;a href=&#34;https://github.com/jpiaskowski/pycas2020_web_scraping&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;div id=&#34;here-is-a-great-way-to-learn-python&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Here is a Great Way to Learn Python!&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;webscraping-book.png&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;But in truth, who actually reads these A to Z?? (spoiler: not me) Many people find these instructional books useful, but as a new Python user, I do not typically read this books front to back. I learn by doing a project, struggling, figuring some things out, and then reading a book like that! (it’s probably time I read that book, actually).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;luke_lightsaber_throwaway.gif&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption&gt;
me and my programming books
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This post provides a guide to my first scraping project in Python. It is very low on assumed knowledge in Python and html. This is intended to be a guide into how to access web page content with the library &lt;strong&gt;requests&lt;/strong&gt; and parse the content using &lt;strong&gt;BeatifulSoup4&lt;/strong&gt;, as well as &lt;strong&gt;json&lt;/strong&gt; and &lt;strong&gt;pandas&lt;/strong&gt;. I will briefly introduce &lt;strong&gt;Selenium&lt;/strong&gt;, but I will not delve deeply into how to use that library (that topic deserves its own tutorial). I hope to show you some tricks and tips to make web scraping less overwhelming.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-requirements-for-a-successful-web-scraping-project&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The Main Requirements for a Successful Web Scraping Project:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Information that is worth the effort it takes to build a working web scraper&lt;/li&gt;
&lt;li&gt;Information that can be legally and ethically gathered by a web scraper&lt;/li&gt;
&lt;li&gt;The tools available in the libraries &lt;code&gt;BeautifulSoup&lt;/code&gt; and &lt;code&gt;requests&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Knowledge on how to find the target information in html code&lt;/li&gt;
&lt;li&gt;Knowledge on how to parse json objects (I will use &lt;code&gt;json&lt;/code&gt; in this tutorial).&lt;/li&gt;
&lt;li&gt;Rudimentary &lt;code&gt;pandas&lt;/code&gt; skills&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A comment on html:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While html is the beast than runs the internet, what we mostly need to understand is how tags work. A tag is a collection of information sandwiched between angle-bracket enclosed labels. Here is my pretend tag, called “pro-tip”:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;pro-tip&amp;gt; All you need to know about html is how tags work &amp;lt;/pro-tip&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can access the information in there (“All you need to know…”) by calling its tag, “pro-tip”. How to find and access a tag will be addressed further in this tutorial.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-look-for-in-a-scraping-project&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What to Look for in a Scraping Project:&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No public API is available for the data. This would be much easier to use and would help clarify both the legality and ethics of gathering the data.&lt;/li&gt;
&lt;li&gt;There needs to be a sizable amount of structured data with a regular repeatable format to justify this effort. Web scraping can be a pain. &lt;strong&gt;BeautifulSoup&lt;/strong&gt; makes this easier, but there is no avoiding the individual idiosyncrasies of websites that will require customisation.&lt;/li&gt;
&lt;li&gt;Identical formatting of the data is not required, but it does make things easier The more “edge cases” (departures from the norm) present, the more complicated the scraping will be.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;spidermen.jpg&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;legal-ethical-considerations&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Legal &amp;amp; Ethical Considerations&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;(note: I have zero legal training - this is not legal advice!)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Accessing vast troves of information can be intoxicating.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Just because it’s possible doesn’t mean it should be done&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Most websites have a &lt;a href=&#34;https://www.contentkingapp.com/academy/robotstxt/&#34;&gt;robots.txt&lt;/a&gt; file associated with the site indicating which scraping activities are permitted and which are not. It’s largely there for interacting with search engines (the ultimate web scrapers). However, much of the information on websites is considered public information. As such, some consider the robot.txt file as a set of &lt;em&gt;instructions&lt;/em&gt; rather than a legally binding document. The robots.txt file does not address topics such as ethical gathering and usage of the data.&lt;/p&gt;
&lt;p&gt;Questions I ask myself before before beginning a scraping project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Am I scraping copyrighted material?&lt;/li&gt;
&lt;li&gt;Will my scraping activity compromise individual privacy?&lt;/li&gt;
&lt;li&gt;Am I making a large number of request that may overload or damage a server?&lt;/li&gt;
&lt;li&gt;Is it possible the scraping will expose intellectual property I do not own?&lt;/li&gt;
&lt;li&gt;Are there terms of service governing use of the website and am I following those?&lt;/li&gt;
&lt;li&gt;Will my scraping activities diminish the value of the original data? (for example, do I plan to repackage the data as is and perhaps siphon off website traffic from the original source?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When I scrap a site, I make sure I can answer “No” to all of those questions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other resources on this Subjecy:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/publication/324907302_Legality_and_Ethics_of_Web_Scraping&#34;&gt;Krotov and Silva, 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3221625&#34;&gt;Sellars 2-18&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;now-its-time-to-scrape&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now It’s Time to Scrape!&lt;/h3&gt;
&lt;p&gt;My goal was to extract addresses for all Family Dollar stores in Idaho. These have an out sized presence in rural areas, so I wanted to understand how many there are in Idaho and their locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;familydollar1.png&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;The starting point: &lt;a href=&#34;https://locations.familydollar.com/id/&#34; class=&#34;uri&#34;&gt;https://locations.familydollar.com/id/&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;step-1-load-the-libraries&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1: Load the Libraries&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import requests # for making standard html requests
from bs4 import BeautifulSoup # magical tool for parsing html data
import json # for parsing data
from pandas import DataFrame as df # premier library for data organization&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-request-data-from-target-url&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2: Request Data from Target URL&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;page = requests.get(&amp;quot;https://locations.familydollar.com/id/&amp;quot;)
soup = BeautifulSoup(page.text, &amp;#39;html.parser&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Beautiful Soup will take html or xml content and transform it into a complex tree of objects. Here are several common object types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;BeautifulSoup&lt;/code&gt; - the soup (the parsed content)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tag&lt;/code&gt; - a standard html tag, the main type of bs4 element you will encounter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NavigableString&lt;/code&gt; - a string of text within a tag&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Comment&lt;/code&gt; - special type of NavigableString&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More on&lt;/strong&gt; &lt;code&gt;requests.get()&lt;/code&gt; &lt;strong&gt;output:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’ve only used &lt;code&gt;page.text()&lt;/code&gt; to translate the requested page into something readable, but there are other output types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;page.text()&lt;/code&gt; for text (most common)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;page.content()&lt;/code&gt; for byte-by-byte output&lt;/li&gt;
&lt;li&gt;&lt;code&gt;page.json()&lt;/code&gt; for json objects&lt;/li&gt;
&lt;li&gt;&lt;code&gt;page.raw()&lt;/code&gt; for the raw socket response (no thank you)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have only worked on English-only sites using the Latin alphabet. The default encoding settings in &lt;strong&gt;requests&lt;/strong&gt; have worked fine for that. However, there is a rich rich internet world beyond English-only sites. To ensure that &lt;strong&gt;requests&lt;/strong&gt; correct parses the content, you can set the encoding for the text:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;page = requests.get(URL)
page.encoding = &amp;#39;ISO-885901&amp;#39; 
soup = BeautifulSoup(page.text, &amp;#39;html.parser&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;**More on BS4 tags:*&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bs4 element ‘tag’ is capturing an html tag&lt;/li&gt;
&lt;li&gt;it has both a name and attributes that can be accessed like a dictionary: &lt;code&gt;tag[&#39;someAttribute&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;if a tag has multiple attributes with the same name, only the first instance is accessed&lt;/li&gt;
&lt;li&gt;a tag’s children is accessed via &lt;code&gt;tag.contents&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;all tag descendants can be accessed with &lt;code&gt;tag.descendants&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;you can always access the full contents as a stringwith: &lt;code&gt;re.compile(&amp;quot;your_string&amp;quot;)&lt;/code&gt; instead of navigating the html tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-determine-how-to-extract-relevant-content-from-bs4-soup&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 3: Determine How to Extract Relevant Content from bs4 Soup&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;This process can be frustrating.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ren_throws_fit.gif&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be a daunting process filled with missteps. I think the the best way to approach this is start with one representative example and then scale up (this principle is true for any programming task). Viewing the page’s html source code is essential. There are a number of way to do this.&lt;/p&gt;
&lt;p&gt;You can view the entire source code via Python in your IDE (not recommended). Run this code at your own risk:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(soup.prettify())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While printing out the &lt;em&gt;entire source code&lt;/em&gt; for a page might work for a toy example shown in some tutorials, most modern websites have an massive amount of content on any one of their pages. Even the 404 page is likely to be filled with code for headers, footers and so on.&lt;/p&gt;
&lt;p&gt;It is usually easiest to browse the source code via “View Page Source” (right-click –&amp;gt; “view page source”). That most actually the most reliable way find your target content (I will explain why later in this tutorial).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;familydollar2.png&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this instance, I need to find my target content - an address, city, State and zip code - in this vast html ocean. Often, a simple search of the page source (ctrl + F) will yield the section where my target location is located. Once I actually can see an example of my target content (the address for at least one store), I look for an attribute or tag that sets this content apart from the rest.&lt;/p&gt;
&lt;p&gt;It would appear that first, I need to collect web addresses for different cities in Idaho with Family Dollar Stores and visit those websites to get the address information. These web addresses all appear to be enclosed in a ‘href’ tag. Great! I will try searching for that using the &lt;code&gt;find_all&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dollar_tree_list = soup.find_all(&amp;#39;href&amp;#39;)
dollar_tree_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## []&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Searching for ‘href’ did not yield anything (darn). This might have failed because ‘href’ is nested inside the class ‘itemlist’. For the next attempt, search on ‘item_list’. Because “class” is a reserved word in Python, &lt;code&gt;class_&lt;/code&gt; is used instead. The bs4 function &lt;code&gt;soup.find_all()&lt;/code&gt; turned out to be the Swiss army knife of bs4 functions.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dollar_tree_list = soup.find_all(class_ = &amp;#39;itemlist&amp;#39;)
for i in dollar_tree_list[:2]:
  print(i)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;div class=&amp;quot;itemlist&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;ga_w2gi_lp&amp;quot; data-gaact=&amp;quot;Click_to_CityPage&amp;quot; data-galoc=&amp;quot;Aberdeen - ID&amp;quot; dta-linktrack=&amp;quot;City index page - Aberdeen&amp;quot; href=&amp;quot;https://locations.familydollar.com/id/aberdeen/&amp;quot;&amp;gt;Aberdeen&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;
## &amp;lt;div class=&amp;quot;itemlist&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;ga_w2gi_lp&amp;quot; data-gaact=&amp;quot;Click_to_CityPage&amp;quot; data-galoc=&amp;quot;American Falls - ID&amp;quot; dta-linktrack=&amp;quot;City index page - American Falls&amp;quot; href=&amp;quot;https://locations.familydollar.com/id/american-falls/&amp;quot;&amp;gt;American Falls&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anecdotally, I found that searching for a specific class was often a successful approach. Here is more information on that object:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;type(dollar_tree_list)
len(dollar_tree_list)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The content from this BeautifulSoup “ResultSet” can be extracted using &lt;code&gt;.contents&lt;/code&gt;. This is also a good time to create a single representative example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;example = dollar_tree_list[2] # a representative example
example_content = example.contents
print(example_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;lt;a class=&amp;quot;ga_w2gi_lp&amp;quot; data-gaact=&amp;quot;Click_to_CityPage&amp;quot; data-galoc=&amp;quot;Arco - ID&amp;quot; dta-linktrack=&amp;quot;City index page - Arco&amp;quot; href=&amp;quot;https://locations.familydollar.com/id/arco/&amp;quot;&amp;gt;Arco&amp;lt;/a&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;.attr&lt;/code&gt; to find what attributes are present in the contents of this object.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: &lt;code&gt;contents&lt;/code&gt; usually return a list of exactly one item, so the first step is to index that item.&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;example_content = example.contents[0]
example_content.attrs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {&amp;#39;class&amp;#39;: [&amp;#39;ga_w2gi_lp&amp;#39;], &amp;#39;data-galoc&amp;#39;: &amp;#39;Arco - ID&amp;#39;, &amp;#39;data-gaact&amp;#39;: &amp;#39;Click_to_CityPage&amp;#39;, &amp;#39;dta-linktrack&amp;#39;: &amp;#39;City index page - Arco&amp;#39;, &amp;#39;href&amp;#39;: &amp;#39;https://locations.familydollar.com/id/arco/&amp;#39;}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I can see that ‘href’ is an attribute, that can be extracted like a dictionary item:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;example_href = example_content[&amp;#39;href&amp;#39;]
print(example_href)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## https://locations.familydollar.com/id/arco/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-put-it-all-together&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 4: Put it all together:&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;city_hrefs = [] # initialise empty list

for i in dollar_tree_list:
    cont = i.contents[0]
    href = cont[&amp;#39;href&amp;#39;]
    city_hrefs.append(href)

#  check to be sure all went well
for i in city_hrefs[:2]:
  print(i)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## https://locations.familydollar.com/id/aberdeen/
## https://locations.familydollar.com/id/american-falls/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; a list of URL’s of Family Dollar stores in Idaho to scrape.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repeat-steps-1-4-for-the-city-urls&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Repeat Steps 1-4 for the City URLs&lt;/h4&gt;
&lt;p&gt;I still don’t have address information! Now, each city URL needs to scraped to get this information. So, restart the process, using a single, representative example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;page2 = requests.get(city_hrefs[2]) # again establish a representative example
soup2 = BeautifulSoup(page2.text, &amp;#39;html.parser&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;lt;img src=“familydollar3.png” style=&amp;quot;width:100%&amp;gt;&lt;/p&gt;
&lt;p&gt;The address information is nested within ‘type=“application/ld+json”’. After doing a lot of geolocation scraping, I’ve come to recognize this as a common structure for storing address information. Fortunately, &lt;code&gt;soup.find_all()&lt;/code&gt; also enables searching on ‘type’.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arco = soup2.find_all(type=&amp;quot;application/ld+json&amp;quot;)
print(arco[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;script type=&amp;quot;application/ld+json&amp;quot;&amp;gt;
##  {
##    &amp;quot;@context&amp;quot;:&amp;quot;https://schema.org&amp;quot;,
##    &amp;quot;@type&amp;quot;:&amp;quot;Schema Business Type&amp;quot;,
##    &amp;quot;name&amp;quot;: &amp;quot;Family Dollar #9143&amp;quot;,
##    &amp;quot;address&amp;quot;:{
##      &amp;quot;@type&amp;quot;:&amp;quot;PostalAddress&amp;quot;,
##      &amp;quot;streetAddress&amp;quot;:&amp;quot;157 W Grand Avenue&amp;quot;,
##      &amp;quot;addressLocality&amp;quot;:&amp;quot;Arco&amp;quot;,
##      &amp;quot;addressRegion&amp;quot;:&amp;quot;ID&amp;quot;,
##      &amp;quot;postalCode&amp;quot;:&amp;quot;83213&amp;quot;,
##      &amp;quot;addressCountry&amp;quot;:&amp;quot;US&amp;quot;
##    },
##    &amp;quot;containedIn&amp;quot;:&amp;quot;&amp;quot;,  
##    &amp;quot;branchOf&amp;quot;: {
##      &amp;quot;name&amp;quot;:&amp;quot;Family Dollar&amp;quot;,
##      &amp;quot;url&amp;quot;: &amp;quot;https://www.familydollar.com/&amp;quot;
##    },
##    &amp;quot;url&amp;quot;:&amp;quot;https://locations.familydollar.com/id/arco/29143/&amp;quot;,
##    &amp;quot;telephone&amp;quot;:&amp;quot;208-881-5738&amp;quot;,
##    &amp;quot;image&amp;quot;: &amp;quot;//hosted.where2getit.com/familydollarstore/images/storefront.png&amp;quot;
##  }           
##  &amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The address information is in the second list member! Finally!&lt;/p&gt;
&lt;p&gt;I extracted the contents (from the second list item) using &lt;code&gt;.contents&lt;/code&gt; (this is a good default action after filtering the soup). Again, since the output of contents is a list of length one, I indexed that list item:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arco_contents = arco[1].contents[0]
arco_contents&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;\n\t{\n\t  &amp;quot;@context&amp;quot;:&amp;quot;https://schema.org&amp;quot;,\n\t  &amp;quot;@type&amp;quot;:&amp;quot;Schema Business Type&amp;quot;,\n\t  &amp;quot;name&amp;quot;: &amp;quot;Family Dollar #9143&amp;quot;,\n\t  &amp;quot;address&amp;quot;:{\n\t    &amp;quot;@type&amp;quot;:&amp;quot;PostalAddress&amp;quot;,\n\t    &amp;quot;streetAddress&amp;quot;:&amp;quot;157 W Grand Avenue&amp;quot;,\n\t    &amp;quot;addressLocality&amp;quot;:&amp;quot;Arco&amp;quot;,\n\t    &amp;quot;addressRegion&amp;quot;:&amp;quot;ID&amp;quot;,\n\t    &amp;quot;postalCode&amp;quot;:&amp;quot;83213&amp;quot;,\n\t    &amp;quot;addressCountry&amp;quot;:&amp;quot;US&amp;quot;\n\t  },\n\t  &amp;quot;containedIn&amp;quot;:&amp;quot;&amp;quot;,  \n\t  &amp;quot;branchOf&amp;quot;: {\n\t    &amp;quot;name&amp;quot;:&amp;quot;Family Dollar&amp;quot;,\n\t    &amp;quot;url&amp;quot;: &amp;quot;https://www.familydollar.com/&amp;quot;\n\t  },\n\t  &amp;quot;url&amp;quot;:&amp;quot;https://locations.familydollar.com/id/arco/29143/&amp;quot;,\n\t  &amp;quot;telephone&amp;quot;:&amp;quot;208-881-5738&amp;quot;,\n\t  &amp;quot;image&amp;quot;: &amp;quot;//hosted.where2getit.com/familydollarstore/images/storefront.png&amp;quot;\n\t}\t\t\t\n\t&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, looking good. The format presented here is consistent with the json format (also the type did have “json” in its name). A json object can act like a dictionary with nested dictionaries inside. It’s actually a nice format to work with once you become familiar with it (and it’s certainly much easier to program than a long series of regex commands). Although this structurally looks like a json objects, it is still a bs4 object and needs a formal programmatic conversion to json to be accessed as a json object:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arco_json =  json.loads(arco_contents)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;type(arco_json)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;dict&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(arco_json)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {&amp;#39;@context&amp;#39;: &amp;#39;https://schema.org&amp;#39;, &amp;#39;@type&amp;#39;: &amp;#39;Schema Business Type&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;Family Dollar #9143&amp;#39;, &amp;#39;address&amp;#39;: {&amp;#39;@type&amp;#39;: &amp;#39;PostalAddress&amp;#39;, &amp;#39;streetAddress&amp;#39;: &amp;#39;157 W Grand Avenue&amp;#39;, &amp;#39;addressLocality&amp;#39;: &amp;#39;Arco&amp;#39;, &amp;#39;addressRegion&amp;#39;: &amp;#39;ID&amp;#39;, &amp;#39;postalCode&amp;#39;: &amp;#39;83213&amp;#39;, &amp;#39;addressCountry&amp;#39;: &amp;#39;US&amp;#39;}, &amp;#39;containedIn&amp;#39;: &amp;#39;&amp;#39;, &amp;#39;branchOf&amp;#39;: {&amp;#39;name&amp;#39;: &amp;#39;Family Dollar&amp;#39;, &amp;#39;url&amp;#39;: &amp;#39;https://www.familydollar.com/&amp;#39;}, &amp;#39;url&amp;#39;: &amp;#39;https://locations.familydollar.com/id/arco/29143/&amp;#39;, &amp;#39;telephone&amp;#39;: &amp;#39;208-881-5738&amp;#39;, &amp;#39;image&amp;#39;: &amp;#39;//hosted.where2getit.com/familydollarstore/images/storefront.png&amp;#39;}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In than content is key called ‘address’ that has the desired address information in smaller nested dictionary. This can be retrieved as thus:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arco_address = arco_json[&amp;#39;address&amp;#39;]
arco_address&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {&amp;#39;@type&amp;#39;: &amp;#39;PostalAddress&amp;#39;, &amp;#39;streetAddress&amp;#39;: &amp;#39;157 W Grand Avenue&amp;#39;, &amp;#39;addressLocality&amp;#39;: &amp;#39;Arco&amp;#39;, &amp;#39;addressRegion&amp;#39;: &amp;#39;ID&amp;#39;, &amp;#39;postalCode&amp;#39;: &amp;#39;83213&amp;#39;, &amp;#39;addressCountry&amp;#39;: &amp;#39;US&amp;#39;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-finally-put-it-all-together&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 5: Finally, Put It All Together&lt;/h4&gt;
&lt;p&gt;Iterate over the list store URLs in Idaho:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;locs_dict = [] # initialise empty list

for link in city_hrefs:
  locpage = requests.get(link)   # request page info
  locsoup = BeautifulSoup(locpage.text, &amp;#39;html.parser&amp;#39;) 
      # parse the page&amp;#39;s content
  locinfo = locsoup.find_all(type=&amp;quot;application/ld+json&amp;quot;) 
      # extract specific element
  loccont = locinfo[1].contents[0]  
      # get contents from the bs4 element set
  locjson = json.loads(loccont)  # convert to json
  locaddr = locjson[&amp;#39;address&amp;#39;] # get address
  locs_dict.append(locaddr) # add address to list&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do some final data organisations steps: convert to a pandas data frame, drop the unneeded columns “&lt;span class=&#34;citation&#34;&gt;@type&lt;/span&gt;” and “country”) and check the top 5 rows to ensure that everything looks alright.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;locs_df = df.from_records(locs_dict)
locs_df.drop([&amp;#39;@type&amp;#39;, &amp;#39;addressCountry&amp;#39;], axis = 1, inplace = True)
locs_df.head(n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         streetAddress addressLocality addressRegion postalCode
## 0   111 N Main Street        Aberdeen            ID      83210
## 1     253 Harrison St  American Falls            ID      83211
## 2  157 W Grand Avenue            Arco            ID      83213
## 3     177 Main Street          Ashton            ID      83420
## 4     747 N. Main St.        Bellevue            ID      83313&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Results!!&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;adventures_in_babysitting.gif&#34; style=&#34;width:95%&#34;&gt;
&lt;figcaption&gt;
They are also euphoric about these results!
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Make sure to save results!! (still euphoric)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.to_csv(locs_df, &amp;quot;family_dollar_ID_locations.csv&amp;quot;, sep = &amp;quot;,&amp;quot;, index = False)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-few-words-on-selenium&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Few Words on Selenium&lt;/h3&gt;
&lt;p&gt;(using Walgreens as an example)&lt;/p&gt;
&lt;p&gt;“Inspect Element” provides the code for what is displayed in a browser:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;walgreens1&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;While “View Page Source” - provides the code for what &lt;strong&gt;requests&lt;/strong&gt; will obtain:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;walgreens2&#34; style=&#34;width:100%&#34;&gt;&lt;/p&gt;
&lt;p&gt;When these two don’t agree, there are plugins modifying the source code - so, it should be accessed &lt;em&gt;after&lt;/em&gt; the page has loaded in a browser. &lt;strong&gt;requests&lt;/strong&gt; cannot do that, but &lt;strong&gt;Selenium&lt;/strong&gt; can.&lt;/p&gt;
&lt;p&gt;Selenium requires a web driver to retrieve the content. It actually opens a web browser, and this page content is collected. Selenium is powerful - it can interact with loaded content in many ways (read the documentation). After getting data with &lt;strong&gt;Selenium&lt;/strong&gt;, continue to use &lt;strong&gt;BeautifulSoup&lt;/strong&gt; as before:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;url = &amp;quot;https://www.walgreens.com/storelistings/storesbycity.jsp?requestType=locator&amp;amp;state=ID&amp;quot;
driver = webdriver.Firefox(executable_path = &amp;#39;mypath/geckodriver.exe&amp;#39;)
driver.get(url)
soup_ID = BeautifulSoup(driver.page_source, &amp;#39;html.parser&amp;#39;)
store_link_soup = soup_ID.find_all(class_ = &amp;#39;col-xl-4 col-lg-4 col-md-4&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in-conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Conclusion…&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Be Patient&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;yoda_lightsaber.gif&#34; style=&#34;width:75%&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consult the Manuals&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(these are very helpful)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://beautiful-soup-4.readthedocs.io/en/latest/&#34; class=&#34;uri&#34;&gt;https://beautiful-soup-4.readthedocs.io/en/latest/&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://selenium.dev/&#34; class=&#34;uri&#34;&gt;https://selenium.dev/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&#34;luke_brushesoff_dust.gif&#34; style=&#34;width:95%&#34;&gt;
&lt;figcaption&gt;
~ You, After Becoming a Web Scraping Master ~
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Postscript:&lt;/em&gt; It turns out very few people nowadays have seen “Adventures in Babysitting” and get this reference in this post title. That’s okay. To summarise: while a woman babysits 3 charges for an evening, everything goes catastrophically wrong. Yet, somehow she get everyone home unharmed and have some fun adventures along the way. That felt like an apt metaphor for building these scripts. They take considerable attention, but with careful work, you can build a highly functional web scraper.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Post Postscript:&lt;/em&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;family_dollar_locations.png&#34; style=&#34;width:95%&#34;&gt;
&lt;figcaption&gt;
Dollar Stores in America
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;There are many many dollar stores in America.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bad Data Management: A Star Wars Story</title>
      <link>/post/5_starwars_bad_data/</link>
      <pubDate>Sat, 01 Feb 2020 10:50:02 -0700</pubDate>
      <guid>/post/5_starwars_bad_data/</guid>
      <description>&lt;p&gt;Most of us think of Star Wars as the rebellion versus the empire &amp;ndash; good versus evil with key figures: the Jedi, the Sith, Darth Vader, Luke Skywalker, the Emperor, Rey, Kylo Ren. But, The downfall of the governing bodies was not Darth Vader, Luke Skywalker, Kylo Ren or Palpatine. Actually, Star Wars is a story of really bad data management practices.&lt;/p&gt;
&lt;h4 id=&#34;the-republic&#34;&gt;The Republic:&lt;/h4&gt;
&lt;p&gt;The Jedi maintained decent, well-organised archives in a large library and they even had paid librarians on staff. But any jedi can delete information and no record is left regarding this? For heaven’s sake, an entire planet was removed from said archives That ain’t good. It led to the Clone wars and eventual rise of the Galactic Empire. It also turns out that a person just has to look like a jedi in order 
&lt;a href=&#34;https://starwars.fandom.com/wiki/Holocron_Heist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;to steal secured jedi archives&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;younglings.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;original-galatic-empire&#34;&gt;Original Galatic Empire:*&lt;/h4&gt;
&lt;p&gt;This is where all the bad stuff starts. Initially, they had a good system for managing their vasts amount of data and an off-site backup with a very powerful firewall. However, once that firewall was breached, there was poor security inside - I mean, a dead archivist’s hand could be used to access &lt;em&gt;the entire system&lt;/em&gt;, presumably as a root user. Their storage facility also relied on a rather inconvenient system for accessing data: a massive tower than required a remote-controlled tool for physical retrieval of data files.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;archive_joystick.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool itself required manual operation and apparently the entire facility had no backup generator to maintain secruity and functioning in case of a power loss. Incredibly, there was also no search tool. Data archives had to be &lt;em&gt;browsed&lt;/em&gt; (what???). it’s really quite astonishing that anyone ever found what they needed. Options for sending out the data were also remarkably cumbersome - a satellite dish the size of a Star Destroyer has to manually turned in the correct direction to send a file out (was the Empire capable of automating anything?). I also wonder about the file compression capabilities - seriously, is there any file so large it needs a satellite dish of that size?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;scarif_tower.JPG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;the-first-order&#34;&gt;The First Order:&lt;/h4&gt;
&lt;p&gt;The First Order inherits a number of problems from the original Galatic Empire, most notably insufficient backups resulting in lost information. A map to Luke Skywalker is lost, making it nigh impossible to find him. A map to Exogol had two copies ever made, one of them on a destroyed death star sitting in the middle of raging ocean? It’s amazing these were very found. Without these maps, we have no Sith and no Jedi (maybe not such a bad thing).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;skywalker_map.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The point is that if the Jedi had better data practices, then they might have avoided the rise of the empire. And perhaps if the Empire had better data practices, they may not undergone such a rapid downfall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;death_star_wreckage.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;what-happens-with-good-data-management&#34;&gt;What happens with good data management:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Secure data that can only be modified by authorised users&lt;/li&gt;
&lt;li&gt;Validation of data values to prevent errors&lt;/li&gt;
&lt;li&gt;Data is accessible: it can be accessed in many places, by many, in formats that are easy to use&lt;/li&gt;
&lt;li&gt;Stored data is easy to search and summarise&lt;/li&gt;
&lt;li&gt;There are sufficient backups if something goes wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;wait-this-actually-matters&#34;&gt;Wait! This actually matters!&lt;/h4&gt;
&lt;p&gt;This is more than a story to amuse folks, though. Poor data management is a rampant issue across academia. I have observed that most researchers do not document their data processing in sufficient detail to recreate the process. Who else has read this statement &amp;ldquo;outliers were removed&amp;rdquo; with no details into how those outliers were eliminated or the numerical values of those &amp;ldquo;outliers&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;From a research standpoint this means the analytical procedure is not replicable. That means someone starting with the same raw data will not obtain the same results following the described analtical procedure &amp;ndash; because the description was inadequate. This can also mean data are unreliable. If the orginal version of data become lost as individuals clean up the data and do not document their process, then there is no way to evaluate if the cleaning was correctly done or justified. Data loss is always a risk when files are emailed around rather than shared via a more permanent data storage solution. All of these risks become amplified when an individual leaves a research program and can no longer be reached to resolve data questions.&lt;/p&gt;
&lt;h4 id=&#34;tools-for-better-data-management&#34;&gt;Tools for better data management&lt;/h4&gt;
&lt;p&gt;Handily, many many tools exist to assist in data management. I give a bare bones outline in this 
&lt;a href=&#34;/post/workflow_basics/&#34;&gt;post&lt;/a&gt;
 (please read this!!). Below is an even more bare &amp;lsquo;bare bones&amp;rsquo; outline.&lt;/p&gt;
&lt;p&gt;One of the best solutions is to put your data in a database. This a great tool for large quantities of data that follow a regular format and are frequently updated - such as variety testing data.&lt;/p&gt;
&lt;p&gt;However, most agricultural researchers are not so lucky to have such structured data. At the very least, maintaining research outputs in a shared and secured location is a good start. Archiving raw versions of data is paramount. This maybe never be needed, but if it is needed, there is not replacement if those data are lost. Requiring complete documentation from all research contributors on the data set generation and processing is also good. A &amp;ldquo;codebook&amp;rdquo; is another great tool. This is a file accompanying a data set that provides the name of each column, what data are present in that column, and optionally, the units of those data and the range of values expected. If a column contains contains numeric values btweeen 0 and 100, then others will know that 980 is probably an error.&lt;/p&gt;
&lt;p&gt;Most of the researchers I work with were trained in a different era - when data was expensive to obtain and as a result, there was considerably less data to deal with. I count myself among this group. At that time, data management tools were less relevant because we had the time to comprehensively examine a data set for errors. Furthermore, data sharing and data reuse were not widely done. But, we all now those days are long over. We are now expected to process a large quantity of data rapidly, analyse it and eventually arhive it publicaly for possible reuse by someone else. Engaging in practices that enable quality data management is essential for participation in today&amp;rsquo;s research environment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Genomic heritability estimates in sweet cherry reveal non-additive genetic variance is relevant for industry-prioritized traits.</title>
      <link>/publication/cherry_gebv/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/cherry_gebv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Near-infrared calibration of soluble stem carbohydrates for predicting drought tolerance in spring wheat</title>
      <link>/publication/nir_calib/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/nir_calib/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Prediction of Genetic Value for Sweet Cherry Fruit Maturity among Environments Using a 6K SNP Array.</title>
      <link>/publication/sweet_cherry_maturity/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/sweet_cherry_maturity/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quantitative Trait Loci for Cold Tolerance in Chickpea.</title>
      <link>/publication/chickpea_qtl_cold/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/chickpea_qtl_cold/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What I learned at the UseR! conference</title>
      <link>/post/2_user_2018/</link>
      <pubDate>Wed, 18 Jul 2018 10:50:02 -0700</pubDate>
      <guid>/post/2_user_2018/</guid>
      <description>&lt;h4 id=&#34;top-lessons-from-r-users-conference-held-july-10---13-in-brisbane-aus&#34;&gt;Top Lessons from R users conference held July 10 - 13 in Brisbane, AUS&lt;/h4&gt;
&lt;p&gt;With 900 registrants and dozens of talks, there is much to report (
&lt;a href=&#34;https://www.youtube.com/channel/UC_R5smHVXRYGhZYDJsnXTwg/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;videos of most talks&lt;/a&gt;
 provided by R Consortium YouTube channel) I&amp;rsquo;ll skip loads of it and just focus on the top 10 cool stuff.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The hex wall was just straight up cool. Here&amp;rsquo;s the 
&lt;a href=&#34;https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code for that&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I participated in a half day workshop on Rcpp (
&lt;a href=&#34;https://www.youtube.com/watch?v=FZ0LcJbxPF0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 and 
&lt;a href=&#34;https://www.youtube.com/watch?v=EXGhR-kyjRg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
). This continues to be a very popular suite of packages to help users increase the speed and efficiency of their programs. I say &amp;ldquo;suite&amp;rdquo; because in addition to 3 major functions provided by Rcpp, a number of accessory packages have been written to extend its functionality. Several of these accessory packages essentially are providing templates for invoking C++ commands without having to actually know C. Dirk Eddelbuttel introduced the 3 major functions for extending R with C/C++:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;evalCpp: for evaluating ad hoc anonymous expressions&lt;/li&gt;
&lt;li&gt;cppFunction: standard function written in C/C++ and invoked in a program&lt;/li&gt;
&lt;li&gt;sourceCpp: source C/C++ objects &amp;amp; functions from external file
These functions must be compiled each time a fresh R session is started. Building an Rcpp package avoids this and is easy to do:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Rcpp.package.skeleton(&amp;quot;mypackage&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;The other useful workshop I participated in was methods for speeding up R, delivered by Thomas Lumley, formerly of UW and now of U of Auckland. He focused on a few major recommendations:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vectorise whenever possible. This also implies that you know your vectorised functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lapply/apply/sapply/vapply/tapply&lt;/li&gt;
&lt;li&gt;rowMeans, rowSums&lt;/li&gt;
&lt;li&gt;colMeans, colSums&lt;/li&gt;
&lt;li&gt;Consider parallelised functions: mclapply&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Base functions are often designed to handle wildly different input. It may make sense in some instances to rewrite functions, making assumptions (e.g. about the input data) that fit your circumstances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data frames should be avoided if possible - they are expensive to create and often copied in whole when modified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using an optimized matrix algebra library (not LAPACK) may be worth the time to install that. These libraries have optimized how data is accessed from disk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some rec&amp;rsquo;s on how to handle large objects that reasonably don&amp;rsquo;t fit in memory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;buy a computer with more memory (not always a reasonable option)&lt;/li&gt;
&lt;li&gt;put your data in database and access with tools like dbplyr. One recommendation is MonetDBLite, a column-optimized database good for scientific applications for that reason.&lt;/li&gt;
&lt;li&gt;Use special file formats for bigger data sets: HDF5, GitLFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But, first and foremost, he emphasized the use of profiling tools and benchmarking functions. find out where the bottleneck in your program actually are! And be careful to not waste too much time on optimizing code - time saved may not be worth time invested. R is automatically running a JIT (just in time) compiler that makes code run faster each time it is called. It makes sense to turn this off while doing benchmarking:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;enableJIT(0)

# to turn back on:

enableJIT(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Course notes available on his github repo 
&lt;a href=&#34;https://github.com/tslumley/useRfasteR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UseRfasteR&lt;/a&gt;
.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent keynotes all-around. 
&lt;a href=&#34;https://www.youtube.com/watch?v=27FxbDtCFoc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steph da Silva&lt;/a&gt;
 emphasized the importance of &lt;em&gt;community&lt;/em&gt; in open-source community - what that can look like, how to contribute  how your contribution helps develop camaraderie to support sharing of code and development of analysts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=5033jBHFiHE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Peng discussed development of R&lt;/a&gt;
 as it strives to fit both programmers and regular ole scientists analyzing their data. He summarized his own talk quite well in this essay. He described the rise of &amp;ldquo;worse is better&amp;rdquo;, that is, the simplying of options in R to make it easier for new users to learn. The tidyverse is considered such an example. However, as a long-time user, I find dplyr&amp;rsquo;s group-and-summarise options (among others) to be incredibly handy. And I will never go back to using reshape() now that we have gather() and spread(). Even reshape2() is a massive improvement in usability!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jenny Bryan&amp;rsquo;s talk on 
&lt;a href=&#34;https://youtu.be/7oyiPBjLAWY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Code Smells and Feels&amp;rdquo;&lt;/a&gt;
 provided a great introduction to how to identify and fix poorly written code. According to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Code_smell&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
, &amp;ldquo;Code smells are usually not bugs; they are not technically incorrect and do not prevent the program from functioning. Instead, they indicate weaknesses in design that may slow down development or increase the risk of bugs or failures in the future.&amp;rdquo;
Based on the book, &lt;em&gt;Refactoring: Improving the Design of Existing Code&lt;/em&gt;, she gave a few simple directives to create simple code that is easy to understand, debug, and maintain. Hooray!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Hoist entry or exit conditionals to the top of functions&lt;/li&gt;
&lt;li&gt;Use functions as much as possible&lt;/li&gt;
&lt;li&gt;Avoid overly nested code (e.g. a long cascade of nested &amp;ldquo;if&amp;rdquo; statements)&lt;/li&gt;
&lt;li&gt;Not every &amp;ldquo;if&amp;rdquo; needs an else&lt;/li&gt;
&lt;li&gt;Consider implementing true object-oriented programming when working with classes&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t&amp;rsquo; be afraid to write short helper functions
There was also a brief discussion on how to tell a person their code smells bad (&amp;ldquo;that&amp;rsquo;s what we have funny names like &amp;lsquo;excessive use of literals&amp;rsquo;&amp;quot;).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent set of talks on data handling and workflow. In particular, 
&lt;a href=&#34;https://www.youtube.com/watch?v=GrqM2VqIQ20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a talk on the website builder&lt;/a&gt;
 for documenting data analysis called 
&lt;a href=&#34;https://github.com/jdblischak/workflowr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;workflowr&amp;rdquo;&lt;/a&gt;
. 
&lt;a href=&#34;https://www.youtube.com/watch?v=IYfZ6kd7aT0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discussion on consistent workflows&lt;/a&gt;
, recommended conventions for naming variables, creating consistent set of directories for each project, using version controls. Obvious to a data scientist, but rarely used by regular scientist. This resonates strongly with me due to ongoing issues with data management in the agricultural sciences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=3Bu7QUxzIbA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Excellent talk on precision in R&lt;/a&gt;
. Use .Machine to find out more, but basically R is only good to 16 decimal points. Subtraction of one number similar to it can result in cancellation where the result essentially slides to zero, losing precision. Ways to avoid this include working in log space, perhaps adding a zero if needed: log(x + 1), -log(1 - exp(x)). Also, there is the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/Rmpfr/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmpfr package&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=XQmBcpQl8K8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glue package&lt;/a&gt;
 - for &amp;ldquo;gluing&amp;rdquo; strings to data, like string interpolation in bash (&amp;ldquo;$&amp;rdquo;) or python f&amp;rdquo;{&amp;hellip;}&amp;quot;. R does have sprintf() with with identical functionality to C&amp;rsquo;s printf(), but glue makes this easier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=L6FawdEA3W0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fun talk on vwline package&lt;/a&gt;
 for creating variable-width lines, like in Menaurd&amp;rsquo;s famous plot of Napolean&amp;rsquo;s march through Russia.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;./napoleon.jpg&#34; alt=&#34;napolean_plot&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Crossing the finish line: how to develop diagnostic DNA tests as breeding tools after QTL discovery</title>
      <link>/publication/dna_testing/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/publication/dna_testing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Set Up a Virtual Machine for Data Analysis If You Are an IT Admin Noob</title>
      <link>/post/1_vm_wsu/</link>
      <pubDate>Sun, 18 Mar 2018 10:50:02 -0700</pubDate>
      <guid>/post/1_vm_wsu/</guid>
      <description>&lt;h2 id=&#34;basic-info&#34;&gt;Basic Info&lt;/h2&gt;
&lt;p&gt;I came back from PAG (Plant &amp;amp; Animal Genome - a huge ag genetics conference held in San Diego each January) all fired about setting up a virtual machine (VM) to facilitate collaboration. The hope what this would enable us to share files and scripts better (without the aid of countless dropboxes everyone forgets about), and improve overall collaboration.&lt;/p&gt;
&lt;p&gt;I had considered this before - enterprise deployment of RStudio server or Jupyter hub, but our program doesn’t have dedicated IT support to do this, and frankly, it all seemed rather intimidating to a plant geneticist like me. But this would be useful. I had once run my laptop on dual boot with Ubuntu and Windows. I could handle this, couldn’t I? All I want is a VM that everyone can access via remote desktop for their analytical needs. Fortunately, I had no idea how difficult this would be, or I might not have entered the lion’s den. So, here are fruits of my labor in the hopes it will ease the process for future users. This guide was developed specific for Washington State University&amp;rsquo;s College of Agriculture, Human and Natural Resource Sciences (CAHNRS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt;  Request a virtual machine. I used the 
&lt;a href=&#34;https://it.cahnrs.wsu.edu/service-catalog/vm-hosting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;CAHNRS webform&lt;/strong&gt;&lt;/a&gt;
. I went with CentOS 7 - a Linux distribution that had been recommended to me as “full featured” (this remains to be seen). This step may take a few weeks.&lt;/p&gt;
&lt;p&gt;It helps to know exactly what you want to do so CAHNRS IT can properly set up the VM and “rules” (exceptions to their standing rules) to allow you to do the work you want. Running RStudio server means having at least one port open. The RStudio Server default port is 8787 (although than can be easily changed). Remote desktop needs port 3350 open. Jupyter Hub will have its own conventions - all described in their documentation. I recommend you consult the documentation for what you want &lt;em&gt;before&lt;/em&gt; ordering a VM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Download the 
&lt;a href=&#34;https://its.wsu.edu/ssl-vpn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;WSU VPN client&lt;/strong&gt;&lt;/a&gt;
. Whoever is the admin will need have the WSU SSLVPN to tunnel in through a secure connection in order to do any work remotely. This is the probably the easiest step of the entire process. Who doesn’t love point-and-click installations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Download software for SSH. Initially, you will need an SSH program to connect through secure, encrypted tunnel. 
&lt;a href=&#34;https://www.chiark.greenend.org.uk/~sgtatham/putty/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Putty&lt;/strong&gt;&lt;/a&gt;
 is a popular option. I like secure shell, which I found 
&lt;a href=&#34;https://www.wm.edu/offices/it/services/software/licensedsoftware/webeditingsftp/sshsecureshell/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;
. This is actually a rather convenient way to manage the VM, so its utility will likely stretch past the setup phase.&lt;/p&gt;
&lt;h2 id=&#34;general-linux-notes&#34;&gt;General Linux notes:&lt;/h2&gt;
&lt;p&gt;This is CentOS, an alternative distribution to its more popular relative, Ubuntu. While most commands are the same across Linux distributions, some things are a little different. FYI, Centos is similar to .rhel and Red Hat if you are combing forums for help.&lt;/p&gt;
&lt;p&gt;Check out these 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;standards for Linux filesystems&lt;/strong&gt;&lt;/a&gt;
. It will help you understand how the machine is organized. Don’t spend too much time here; it’s more of a reference than pleasure reading.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.centos.org/docs/5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Centos documentation&lt;/strong&gt;&lt;/a&gt;
 can be helpful, as well.&lt;/p&gt;
&lt;h3 id=&#34;miscellaneous-useful-commands&#34;&gt;Miscellaneous useful commands:&lt;/h3&gt;
&lt;p&gt;Most of the command listed here for VM setup require root privileges, which can be accomplished with “sudo”. You’ll find you need to be root 98% of time during VM setup, so do this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo su&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This changes the console from:
&lt;code&gt;$ &lt;/code&gt;
to:
&lt;code&gt;#   &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Use Ctrl-D to leave root. And be careful when you are root!&lt;/p&gt;
&lt;p&gt;Print current working directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pwd&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Change the working directory to go up a level:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd .. &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Absolute path (will work everywhere):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$ cd /&amp;lt;path&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Relative path (only sees child directories):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This produces a long list of all files and directories in your current working directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ll -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To see options associated with a particular command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;command&amp;gt; --help&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To remove a single file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rm &amp;lt;your_file.txt&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To remove an entire directory:&lt;br&gt;
‘r’ does it recursively&lt;br&gt;
‘f’ will do it without asking again (it skips the step that is essentially asking “are you sure you want to delete this????”)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rm -rf &amp;lt;your_directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To install a piece of software for all users:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo yum install &amp;lt;library&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To view files:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;more &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To edit files. This command requires sudo for most of the files you will need to look at for VM setup.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nano &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There’s also the vim editor, but use it your own risk. Here is the best description I have ever read of it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If you want an outstanding free text editor and don&amp;rsquo;t mind a seemingly vertical learning curve plus long days of pain and suffering while all your neural pathways are rewired, try Vim.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;vi &amp;lt;file&amp;gt; &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To escape vim:&lt;/p&gt;
&lt;p&gt;&lt;code&gt; :q &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Make a new directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mkdir &amp;lt;dir_name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Setting your VM to the correct timezone (Pacific in this case). It is helpful to have the correct time when you are inspecting the logs during troubleshooting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;timedatectl set-timezone &#39;America/Los_Angeles&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;managing-users&#34;&gt;Managing Users&lt;/h3&gt;
&lt;p&gt;It is important to think thru how to organize the users into groups and create folders with shared permissions in order to enable collaboration.&lt;/p&gt;
&lt;p&gt;Below are some common commands. Note that they all require root or sudo access. Centos provides excellent 
&lt;a href=&#34;https://www.centos.org/docs/5/html/Deployment_Guide-en-US/s1-users-tools.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;
 on this.&lt;/p&gt;
&lt;p&gt;Add a new user (the second option adds a new user to an existing group):&lt;br&gt;
&lt;code&gt;useradd &amp;lt;username&amp;gt;&lt;/code&gt;&lt;br&gt;
&lt;code&gt;useradd &amp;lt;username&amp;gt; -g &amp;lt;groupname&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Set user password:&lt;br&gt;
&lt;code&gt;passwd &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Add a group:&lt;br&gt;
&lt;code&gt;groupadd &amp;lt;groupname&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Add an existing user to an existing group:&lt;br&gt;
&lt;code&gt;sudo usermod -a -G &amp;lt;groupname&amp;gt; &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Enable file ownership by a group:&lt;br&gt;
&lt;code&gt;chown :&amp;lt;groupname&amp;gt; &amp;lt;path/to/directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Enable file ownership by a user:&lt;br&gt;
&lt;code&gt;chown :&amp;lt;username&amp;gt; &amp;lt;path/to/directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Delete a user:&lt;br&gt;
&lt;code&gt;userdel &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Link directories:&lt;br&gt;
&lt;code&gt;ln -s &amp;lt;path/to/file&amp;gt; &amp;lt;path/to/link&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;if-you-want-to-run-remote-desktop&#34;&gt;If you want to run remote desktop:&lt;/h2&gt;
&lt;p&gt;There are other desktop environments, but Gnome is a popular one, so why not?&lt;/p&gt;
&lt;p&gt;See what is available:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum group list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install Gnome desktop environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install gnome
sudo yum install gnome desktop environment
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check that it is running&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ps -aux | grep gnome
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install and configure the EPEL repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the nux repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install xrdp:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum -y install xrdp tigervnc*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll need to start xrdp and xrdp-sesman (“session manager”) as a service (so they are always running) and enable them both to autostart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl start xrdp
sudo systemctl enable xrdp
sudo systemctl start xrdp-sesman
sudo systemctl enable xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And check:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl status xrdp
sudo systemctl status xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These should indicate that xrdp and xrdp-sesman are &lt;em&gt;active&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Open a port to allow remote desktop connections:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo firewall-cmd --permanent --zone=public --add-port=3389/tcp
sudo firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Configure SELinux:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo chcon --type=bin_t /usr/sbin/xrdp
sudo chcon --type=bin_t /usr/sbin/xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And check that the port is open:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;netstat -plan | grep xrdp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, vnc has to be started. It is tied to a user, so get out of root if you are there (and don’t use sudo).&lt;/p&gt;
&lt;p&gt;Set a password:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vncpasswd -&amp;lt;user&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It will return a prompt, asking you for a password. This is what you will use for remote desktop. You can definitely skip the  step of having a read-only password by answering “n” for no. (It’s unclear to me why anyone would want a password for that as opposed to whole new user account with read-only permissions, but what do I know?).&lt;/p&gt;
&lt;p&gt;Next, start the server. Try to only do this once.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vnc server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you end up with multiple vnc server instances running on just ONE user account, kill the extra processes and delete their log files. Here are the steps:&lt;/p&gt;
&lt;p&gt;First, find the process ID’s:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ps aux | grep vnc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The process ID&amp;rsquo;s (pid) are located in the second column from the left. Kill the extra vnc pid&amp;rsquo;s:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kill -9 &amp;lt;pid&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Find the associated files and delete them. I found there here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /var/log
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They will have names like &amp;ldquo;Xorg.9.log&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;troubleshooting-remote-desktop&#34;&gt;Troubleshooting Remote Desktop&lt;/h3&gt;
&lt;p&gt;If you run into trouble, try disabling SELinux:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo setenforce 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;getenforce
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(it should say “permissive” NOT “enforcing)&lt;/p&gt;
&lt;p&gt;Check for problems (as root):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /var/logs
more xrdp.log
more xrdp-sesman.log
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are cleared rapidly and folded into the xrdp .tar files, so do not be alarmed if the log files are empty. If you really want to see something in the xrdp logs, stop and restart xrdp.&lt;/p&gt;
&lt;p&gt;The most useful file is the messages file, but it’s very long, so just show the end:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo tail -55 messages
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you &lt;em&gt;really&lt;/em&gt; want to see what&amp;rsquo;s going on, watch the file in real time:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tail -f messages&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Ctlr-C to leave&lt;/p&gt;
&lt;p&gt;The firewalld file in the /var/log/ directory is stuffed with known, but currently unresolved bugs so it’s not very useful for diagnosing problems at this time.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a number of remote desktop applications which can be used to access the VM. Windows RDP is easy to use and secure.&lt;/p&gt;
&lt;h3 id=&#34;run-r-in-remote-desktop&#34;&gt;Run R in remote desktop&lt;/h3&gt;
&lt;p&gt;Start by installing R. WSU would not let me access the internet directly from the remote desktop internet browser, but I could through the SSH. So here are command line instructions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install epel-release
sudo yum update
sudo yum install R -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, install RStudio. Get current link from the 
&lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio website&lt;/strong&gt;&lt;/a&gt;
. The link below worked March 19, 2018:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://download1.rstudio.org/rstudio-1.1.442-x86_6
sudo rpm -Uvh rstudio-1.1.442-x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other&#34;&gt;Other&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s also helpful to have an office suite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install libreoffice
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, all done! Go and solve the world’s problems!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perennial wheat lines have highly admixed population structure and elevated rates of outcrossing</title>
      <link>/publication/perennial_wheat/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>/publication/perennial_wheat/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Building supportive networks among agricultural innovators through a symposium on dryland organic farming</title>
      <link>/publication/networks/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      <guid>/publication/networks/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
