[{"authors":["admins"],"categories":null,"content":"Julia Piaskowski is a consulting statistician serving the College of Agriculture and Life Sciences. Her research interests include agricultural statistics, spatial statistics, and quantitative genetics. Lately, she is focused on encouraging reproducible analytical workflows and the safe guarding of data from long term agricultural research projects.\n","date":1587837002,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587837002,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Julia Piaskowski is a consulting statistician serving the College of Agriculture and Life Sciences. Her research interests include agricultural statistics, spatial statistics, and quantitative genetics. Lately, she is focused on encouraging reproducible analytical workflows and the safe guarding of data from long term agricultural research projects.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":" Zotero is one of many software options for managing references. These applications can help you keep track of references you have gathered, organize them into useful categories, and most importantly, be used to format and manage references in a document, saving untold hours of time. Other options are Mendeley and Endnote, among many others. Zotero is a free open-source software is that is very popular and hence enjoys the participation of many developers and journals.\nAdoption of reference software appears to be lagging in horticulture crops. Few horticulture journals have developed a .csl file that can be used by reference software. This will likely change with time.\nOne reason to use reference software is massive author lists:\n","date":1521395402,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1521395402,"objectID":"e6d4d3f2997b1334a8d3df5e59e1fdef","permalink":"/tutorials/zotero/","publishdate":"2018-03-18T10:50:02-07:00","relpermalink":"/tutorials/zotero/","section":"tutorials","summary":"A brief into how to use Zotero for citation management","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Installation Step 1: install Zotero . It now comes automatically bundled with an extension for inserting citations into Microsoft Word and Libre Writer.\nOnce installed, you should be able to open Zotero and see this:\nYou should also consider creating an online account so your local library can sync with the cloud service. This is free. You can link the Zotero app to your account under \u0026ldquo;Zotero\u0026rdquo; \u0026gt; \u0026ldquo;Preferences\u0026rdquo;:\nThe Zotero tab should be present on Microsoft Word. This works on Windows and Mac versions, and it also works with Libre Office.\nThere is also a large number of plugins to consider.\nAdding references Formats Many journals provide options for direct import of a reference into. NCBI, Springer, ASA-CSSA-SSSA and many more are quite good at this.\nOften many formats are offered, each specifc to a particular software. If you see an option for Zotero, clearly that will work fine for importation. Otherwise, a plain .RIS file is the best option. I often choose \u0026ldquo;RefWorks\u0026rdquo; as the format for that program is compatible with Zotero.\nImporting References  Example: Springer. Mapping of a major gene for the slow ripening character in peach: co-location with the maturity date gene and development of a candidate gene-based diagnostic marker for its selection  Example: NCBI. QTL analysis of quality traits in an advanced backcross between Prunus persica cultivars and the wild relative species P. davidiana  Example: PMC Full text. The impact of genetic relationship information on genome-assisted breeding values  Example: ASHS. Variability in sugars, acids, firmness, and color characteristics of 12 peach genotypes  Example: Multiple References. Fixed-length haplotypes can improve genomic prediction accuracy in an admixed dairy cattle population   Manual Addition A references can always be entered by hand under \u0026ldquo;File\u0026rdquo; \u0026gt; \u0026ldquo;New Item\u0026rdquo;. This is laborious and likely to result in errors, so I do not recommend it. However, sometimes there is no choice.\nPDFs and other attachments are usually added manually. Use the paperclip icon to attached local files. An exception is importing an existing library.\nOther Reference Types Zotero offers many reference types. The default item type used by Zotero (and scientists) is \u0026ldquo;Journal Article\u0026rdquo;. However, there are a number of other types to consider. Here is a sampling of other commonly-used item types:\n Book or Book Section   Dissertation   Conference Paper   Webpage  Using these different types enables users to capture the information essential to the particular reference. For citation purposes, journals have developed a set of formatting guidelines for each reference type. By using the correct reference type, references are more likely to be properly formatted within documents.\nEditing References Regardless of how references are added, all references should be inspected once to ensure all elements are as expected. Sometimes, article titles or author names are in all caps. Often, each word in an article title is capitalized. Occasionally, the abstract and title are merged, making for an epically long title. For books, editors may not be included or may be improperly labelled. The citation manager will not be able to correct these issues - it is up to users to do this.\nThe most notable drawback to Zotero is that you will lose italics. This can be ignored in abstracts, but if you plan on citing an article with an italicized word in the title, it is best to fix this. To do so, flank the italized text in the following html code:\n\u0026lt;i\u0026gt; your italicized text \u0026lt;/i\u0026gt;  Which will look like this after inserted into a list of references:  your italized text \nJournal names are not always standardized. This can sometimes be ignored since the Medline abbreviations are used. For example, here are several variations in name of a reputable journal commonly called \u0026ldquo;PNAS\u0026rdquo;:\n PNAS PNAS USA Proc Natl Acad Sci Proc Natl Acad Sci USA Proc. Natl Acad. Sci. USA Proceeding of the National Academy of Sciences Proceedings of the National Academy of Sciences  Manipulating Zotero For dedicated users, a reference library can become quite large and unwieldy. Hence knowelge on how to organize, filter and search a library are essential to taking full advantage of the information stored in personal or shared reference library.\nOrganize References into Collections References can be organized into many collections. Collections or subcollections can be created by right clicking within the left menu and choosing the appropriate option. References are added to collections by highlighting and dropping and dragging them to the appropriate collection. References can belong to multiple collections. Collections are not hierarchical: membership in a subcollection does not guarantee membership in the higher collection.\nGroup collections are a shared resource. These are created on the Zotero website and other zotero users can be invited to join. These can be private or public.\nFinding References You can sort references by several of the fields in the references: date, title author, date added, attachments. References can be searched with the search bar by \u0026ldquo;everything\u0026rdquo; or \u0026ldquo;all fields and tags\u0026rdquo; or \u0026ldquo;Title, creator, date\u0026rdquo;. Tags are the equivalent of \u0026ldquo;keywords\u0026rdquo; used in classifying journal articles. Searches can also be restricted to within collections.\n![zotero_filter](/zotero_filter.png) Citations Automated citation management is perhaps the most useful aspects of a reference software. With Zotero, it\u0026rsquo;s also very easy to use.\nFirst, you need to install the desired citation style. Major journals have made their styles available on the Zotero Style Repository . To download one, you can directly download it from the Zotero style repository webpage and import into Zotero. Alternatively, Zotero provides an option directly in their user interface through \u0026ldquo;Zotero\u0026rdquo; \u0026gt; \u0026ldquo;Preferences\u0026rdquo; \u0026gt; \u0026ldquo;Cite\u0026rdquo;:\n![zotero_styles](zotero_styles.png) Inserting and Editing References\nOpen a word document and try to insert a reference where your cursor is located in the document with \u0026ldquo;Add/Edit Citation\u0026rdquo;. It will prompt you to choose a style the first time you do this.\nTo change the content in a single group of references, place your cursor in the reference and click \u0026ldquo;Add/Edit Citation\u0026rdquo;. You can delete and/or add references.\nReferences can be moved around as whole text or deleted entirely like any normal document text. Zotero will automatically detect their removal and update the bibliography.\nUsers may want to cite papers as such: \u0026ldquo;Coombe (1976) found that\u0026hellip;\u0026quot;. In this example, the default format \u0026ldquo;Coombe (Coombe 1976) found that\u0026hellip;\u0026rdquo; is incorrect. To remove the author name from the citation, choose \u0026ldquo;classic view\u0026rdquo; from the dropdown menu:\nand click \u0026ldquo;suppress author\u0026rdquo;:\nAdding a Bibliography\nTo add a list of ordered cited works to a document, place your cursor where you would like the reference list inserted and click \u0026ldquo;Add/Edit Bibliography\u0026rdquo;. This can be added and deleted repeatedly if needed.\nThe button \u0026ldquo;Document Preferences\u0026rdquo; can be used to change the citation style across the entire document if needed.\nNote that the bibliography formatting will default to the text type defined for the block (usually \u0026ldquo;normal\u0026rdquo;). If you want it to look different (smaller, different font, etc), you will have to redefine that block or the elements of that type.\nUnlink Citations\nThis will remove all links between references, so be careful using this button, as you will have to manually edit references from this point forward. It is probably best to copy your document first.\nTroubleshooting For some reason, normal journals are being formatted as webpages. This can happen if the URL field is filled out. Delete or move that content for the references in question.\nI accidentially added the wrong PDF to the article. Duplicate the reference within Zotero, delete the old reference and add the correct attachment.\nOh snap - my whole library was wiped. What should I do? You should back it up from this point forward. Here are some instructions . There are some file recovery tools for individual systems. There is also a Zotero plugin for pulling references out of your papers.\nI accidentially imported all references from a single article rather than only the citation for that article. Sort the references from the date added and remove all those added at that particular time stamp.\nI think there are tons of duplicates in my library - how can I fix this? Zotero has a feature for removing duplicates. Look on the left menu for \u0026ldquo;Duplicate Items\u0026rdquo; to clean those up.\nHelp! I\u0026rsquo;m having some mystery issue that no one has ever experienced. Zotero has a huge userbase who take advantage of the forums for troubleshooting. If you cannot find your solution there, then post your problem, and the community is likely to help you.\n","date":1521395402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521395402,"objectID":"985edd2192bbac1e47b797fd17d85065","permalink":"/tutorials/zotero/zotero_workshop/","publishdate":"2018-03-18T10:50:02-07:00","relpermalink":"/tutorials/zotero/zotero_workshop/","section":"tutorials","summary":"Installation Step 1: install Zotero . It now comes automatically bundled with an extension for inserting citations into Microsoft Word and Libre Writer.\nOnce installed, you should be able to open Zotero and see this:\nYou should also consider creating an online account so your local library can sync with the cloud service. This is free. You can link the Zotero app to your account under \u0026ldquo;Zotero\u0026rdquo; \u0026gt; \u0026ldquo;Preferences\u0026rdquo;:\nThe Zotero tab should be present on Microsoft Word.","tags":null,"title":"Using Zotero","type":"docs"},{"authors":["Julia Piaskowski"],"categories":[],"content":" This is adapted from a talk I gave at Pycascades in Portland, Oregon 2020-02-09.\nAll the resources from this talk are available at my GitHub repo.\nIntroduction Here is a Great Way to Learn Python! But in truth, who actually reads these A to Z?? (spoiler: not me) Many people find these instructional books useful, but as a new Python user, I do not typically read this books front to back. I learn by doing a project, struggling, figuring some things out, and then reading a book like that! (it’s probably time I read that book, actually).\n  me and my programming books   This post provides a guide to my first scraping project in Python. It is very low on assumed knowledge in Python and html. This is intended to be a guide into how to access web page content with the library requests and parse the content using BeatifulSoup4, as well as json and pandas. I will briefly introduce Selenium, but I will not delve deeply into how to use that library (that topic deserves its own tutorial). I hope to show you some tricks and tips to make web scraping less overwhelming.\n The Main Requirements for a Successful Web Scraping Project:  Information that is worth the effort it takes to build a working web scraper Information that can be legally and ethically gathered by a web scraper The tools available in the libraries BeautifulSoup and requests Knowledge on how to find the target information in html code Knowledge on how to parse json objects (I will use json in this tutorial). Rudimentary pandas skills  A comment on html:\nWhile html is the beast than runs the internet, what we mostly need to understand is how tags work. A tag is a collection of information sandwiched between angle-bracket enclosed labels. Here is my pretend tag, called “pro-tip”:\n\u0026lt;pro-tip\u0026gt; All you need to know about html is how tags work \u0026lt;/pro-tip\u0026gt;\nWe can access the information in there (“All you need to know…”) by calling its tag, “pro-tip”. How to find and access a tag will be addressed further in this tutorial.\n What to Look for in a Scraping Project: No public API is available for the data. This would be much easier to use and would help clarify both the legality and ethics of gathering the data. There needs to be a sizable amount of structured data with a regular repeatable format to justify this effort. Web scraping can be a pain. BeautifulSoup makes this easier, but there is no avoiding the individual idiosyncrasies of websites that will require customisation. Identical formatting of the data is not required, but it does make things easier The more “edge cases” (departures from the norm) present, the more complicated the scraping will be.   Legal \u0026amp; Ethical Considerations (note: I have zero legal training - this is not legal advice!)\nAccessing vast troves of information can be intoxicating.\nJust because it’s possible doesn’t mean it should be done\nMost websites have a robots.txt file associated with the site indicating which scraping activities are permitted and which are not. It’s largely there for interacting with search engines (the ultimate web scrapers). However, much of the information on websites is considered public information. As such, some consider the robot.txt file as a set of instructions rather than a legally binding document. The robots.txt file does not address topics such as ethical gathering and usage of the data.\nQuestions I ask myself before before beginning a scraping project:\n Am I scraping copyrighted material? Will my scraping activity compromise individual privacy? Am I making a large number of request that may overload or damage a server? Is it possible the scraping will expose intellectual property I do not own? Are there terms of service governing use of the website and am I following those? Will my scraping activities diminish the value of the original data? (for example, do I plan to repackage the data as is and perhaps siphon off website traffic from the original source?)  When I scrap a site, I make sure I can answer “No” to all of those questions.\nOther resources on this Subjecy:\n Krotov and Silva, 2018 Sellars 2-18    Now It’s Time to Scrape! My goal was to extract addresses for all Family Dollar stores in Idaho. These have an out sized presence in rural areas, so I wanted to understand how many there are in Idaho and their locations.\nThe starting point: https://locations.familydollar.com/id/\nStep 1: Load the Libraries import requests # for making standard html requests from bs4 import BeautifulSoup # magical tool for parsing html data import json # for parsing data from pandas import DataFrame as df # premier library for data organization  Step 2: Request Data from Target URL page = requests.get(\u0026quot;https://locations.familydollar.com/id/\u0026quot;) soup = BeautifulSoup(page.text, \u0026#39;html.parser\u0026#39;)  Beautiful Soup will take html or xml content and transform it into a complex tree of objects. Here are several common object types:\n BeautifulSoup - the soup (the parsed content)\n Tag - a standard html tag, the main type of bs4 element you will encounter NavigableString - a string of text within a tag\n Comment - special type of NavigableString  More on requests.get() output:\nI’ve only used page.text() to translate the requested page into something readable, but there are other output types:\n page.text() for text (most common) page.content() for byte-by-byte output page.json() for json objects page.raw() for the raw socket response (no thank you)  I have only worked on English-only sites using the Latin alphabet. The default encoding settings in requests have worked fine for that. However, there is a rich rich internet world beyond English-only sites. To ensure that requests correct parses the content, you can set the encoding for the text:\npage = requests.get(URL) page.encoding = \u0026#39;ISO-885901\u0026#39; soup = BeautifulSoup(page.text, \u0026#39;html.parser\u0026#39;)  **More on BS4 tags:*\n The bs4 element ‘tag’ is capturing an html tag it has both a name and attributes that can be accessed like a dictionary: tag['someAttribute'] if a tag has multiple attributes with the same name, only the first instance is accessed a tag’s children is accessed via tag.contents all tag descendants can be accessed with tag.descendants you can always access the full contents as a stringwith: re.compile(\u0026quot;your_string\u0026quot;) instead of navigating the html tree.   Step 3: Determine How to Extract Relevant Content from bs4 Soup This process can be frustrating.\nThis can be a daunting process filled with missteps. I think the the best way to approach this is start with one representative example and then scale up (this principle is true for any programming task). Viewing the page’s html source code is essential. There are a number of way to do this.\nYou can view the entire source code via Python in your IDE (not recommended). Run this code at your own risk:\nprint(soup.prettify()) While printing out the entire source code for a page might work for a toy example shown in some tutorials, most modern websites have an massive amount of content on any one of their pages. Even the 404 page is likely to be filled with code for headers, footers and so on.\nIt is usually easiest to browse the source code via “View Page Source” (right-click –\u0026gt; “view page source”). That most actually the most reliable way find your target content (I will explain why later in this tutorial).\nIn this instance, I need to find my target content - an address, city, State and zip code - in this vast html ocean. Often, a simple search of the page source (ctrl + F) will yield the section where my target location is located. Once I actually can see an example of my target content (the address for at least one store), I look for an attribute or tag that sets this content apart from the rest.\nIt would appear that first, I need to collect web addresses for different cities in Idaho with Family Dollar Stores and visit those websites to get the address information. These web addresses all appear to be enclosed in a ‘href’ tag. Great! I will try searching for that using the find_all command:\ndollar_tree_list = soup.find_all(\u0026#39;href\u0026#39;) dollar_tree_list ## [] Searching for ‘href’ did not yield anything (darn). This might have failed because ‘href’ is nested inside the class ‘itemlist’. For the next attempt, search on ‘item_list’. Because “class” is a reserved word in Python, class_ is used instead. The bs4 function soup.find_all() turned out to be the Swiss army knife of bs4 functions.\ndollar_tree_list = soup.find_all(class_ = \u0026#39;itemlist\u0026#39;) for i in dollar_tree_list[:2]: print(i) ## \u0026lt;div class=\u0026quot;itemlist\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;ga_w2gi_lp\u0026quot; data-gaact=\u0026quot;Click_to_CityPage\u0026quot; data-galoc=\u0026quot;Aberdeen - ID\u0026quot; dta-linktrack=\u0026quot;City index page - Aberdeen\u0026quot; href=\u0026quot;https://locations.familydollar.com/id/aberdeen/\u0026quot;\u0026gt;Aberdeen\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; ## \u0026lt;div class=\u0026quot;itemlist\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;ga_w2gi_lp\u0026quot; data-gaact=\u0026quot;Click_to_CityPage\u0026quot; data-galoc=\u0026quot;American Falls - ID\u0026quot; dta-linktrack=\u0026quot;City index page - American Falls\u0026quot; href=\u0026quot;https://locations.familydollar.com/id/american-falls/\u0026quot;\u0026gt;American Falls\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; Anecdotally, I found that searching for a specific class was often a successful approach. Here is more information on that object:\ntype(dollar_tree_list) len(dollar_tree_list) The content from this BeautifulSoup “ResultSet” can be extracted using .contents. This is also a good time to create a single representative example.\nexample = dollar_tree_list[2] # a representative example example_content = example.contents print(example_content) ## [\u0026lt;a class=\u0026quot;ga_w2gi_lp\u0026quot; data-gaact=\u0026quot;Click_to_CityPage\u0026quot; data-galoc=\u0026quot;Arco - ID\u0026quot; dta-linktrack=\u0026quot;City index page - Arco\u0026quot; href=\u0026quot;https://locations.familydollar.com/id/arco/\u0026quot;\u0026gt;Arco\u0026lt;/a\u0026gt;] Use .attr to find what attributes are present in the contents of this object.\nNote: contents usually return a list of exactly one item, so the first step is to index that item.\nexample_content = example.contents[0] example_content.attrs ## {\u0026#39;class\u0026#39;: [\u0026#39;ga_w2gi_lp\u0026#39;], \u0026#39;data-galoc\u0026#39;: \u0026#39;Arco - ID\u0026#39;, \u0026#39;data-gaact\u0026#39;: \u0026#39;Click_to_CityPage\u0026#39;, \u0026#39;dta-linktrack\u0026#39;: \u0026#39;City index page - Arco\u0026#39;, \u0026#39;href\u0026#39;: \u0026#39;https://locations.familydollar.com/id/arco/\u0026#39;} Now that I can see that ‘href’ is an attribute, that can be extracted like a dictionary item:\nexample_href = example_content[\u0026#39;href\u0026#39;] print(example_href) ## https://locations.familydollar.com/id/arco/  Step 4: Put it all together: city_hrefs = [] # initialise empty list for i in dollar_tree_list: cont = i.contents[0] href = cont[\u0026#39;href\u0026#39;] city_hrefs.append(href) # check to be sure all went well for i in city_hrefs[:2]: print(i) ## https://locations.familydollar.com/id/aberdeen/ ## https://locations.familydollar.com/id/american-falls/ Result: a list of URL’s of Family Dollar stores in Idaho to scrape.\n Repeat Steps 1-4 for the City URLs I still don’t have address information! Now, each city URL needs to scraped to get this information. So, restart the process, using a single, representative example.\npage2 = requests.get(city_hrefs[2]) # again establish a representative example soup2 = BeautifulSoup(page2.text, \u0026#39;html.parser\u0026#39;) \u0026lt;img src=“familydollar3.png” style=\u0026quot;width:100%\u0026gt;\nThe address information is nested within ‘type=“application/ld+json”’. After doing a lot of geolocation scraping, I’ve come to recognize this as a common structure for storing address information. Fortunately, soup.find_all() also enables searching on ‘type’.\narco = soup2.find_all(type=\u0026quot;application/ld+json\u0026quot;) print(arco[1]) ## \u0026lt;script type=\u0026quot;application/ld+json\u0026quot;\u0026gt; ## { ## \u0026quot;@context\u0026quot;:\u0026quot;https://schema.org\u0026quot;, ## \u0026quot;@type\u0026quot;:\u0026quot;Schema Business Type\u0026quot;, ## \u0026quot;name\u0026quot;: \u0026quot;Family Dollar #9143\u0026quot;, ## \u0026quot;address\u0026quot;:{ ## \u0026quot;@type\u0026quot;:\u0026quot;PostalAddress\u0026quot;, ## \u0026quot;streetAddress\u0026quot;:\u0026quot;157 W Grand Avenue\u0026quot;, ## \u0026quot;addressLocality\u0026quot;:\u0026quot;Arco\u0026quot;, ## \u0026quot;addressRegion\u0026quot;:\u0026quot;ID\u0026quot;, ## \u0026quot;postalCode\u0026quot;:\u0026quot;83213\u0026quot;, ## \u0026quot;addressCountry\u0026quot;:\u0026quot;US\u0026quot; ## }, ## \u0026quot;containedIn\u0026quot;:\u0026quot;\u0026quot;, ## \u0026quot;branchOf\u0026quot;: { ## \u0026quot;name\u0026quot;:\u0026quot;Family Dollar\u0026quot;, ## \u0026quot;url\u0026quot;: \u0026quot;https://www.familydollar.com/\u0026quot; ## }, ## \u0026quot;url\u0026quot;:\u0026quot;https://locations.familydollar.com/id/arco/29143/\u0026quot;, ## \u0026quot;telephone\u0026quot;:\u0026quot;208-881-5738\u0026quot;, ## \u0026quot;image\u0026quot;: \u0026quot;//hosted.where2getit.com/familydollarstore/images/storefront.png\u0026quot; ## } ## \u0026lt;/script\u0026gt; The address information is in the second list member! Finally!\nI extracted the contents (from the second list item) using .contents (this is a good default action after filtering the soup). Again, since the output of contents is a list of length one, I indexed that list item:\narco_contents = arco[1].contents[0] arco_contents ## \u0026#39;\\n\\t{\\n\\t \u0026quot;@context\u0026quot;:\u0026quot;https://schema.org\u0026quot;,\\n\\t \u0026quot;@type\u0026quot;:\u0026quot;Schema Business Type\u0026quot;,\\n\\t \u0026quot;name\u0026quot;: \u0026quot;Family Dollar #9143\u0026quot;,\\n\\t \u0026quot;address\u0026quot;:{\\n\\t \u0026quot;@type\u0026quot;:\u0026quot;PostalAddress\u0026quot;,\\n\\t \u0026quot;streetAddress\u0026quot;:\u0026quot;157 W Grand Avenue\u0026quot;,\\n\\t \u0026quot;addressLocality\u0026quot;:\u0026quot;Arco\u0026quot;,\\n\\t \u0026quot;addressRegion\u0026quot;:\u0026quot;ID\u0026quot;,\\n\\t \u0026quot;postalCode\u0026quot;:\u0026quot;83213\u0026quot;,\\n\\t \u0026quot;addressCountry\u0026quot;:\u0026quot;US\u0026quot;\\n\\t },\\n\\t \u0026quot;containedIn\u0026quot;:\u0026quot;\u0026quot;, \\n\\t \u0026quot;branchOf\u0026quot;: {\\n\\t \u0026quot;name\u0026quot;:\u0026quot;Family Dollar\u0026quot;,\\n\\t \u0026quot;url\u0026quot;: \u0026quot;https://www.familydollar.com/\u0026quot;\\n\\t },\\n\\t \u0026quot;url\u0026quot;:\u0026quot;https://locations.familydollar.com/id/arco/29143/\u0026quot;,\\n\\t \u0026quot;telephone\u0026quot;:\u0026quot;208-881-5738\u0026quot;,\\n\\t \u0026quot;image\u0026quot;: \u0026quot;//hosted.where2getit.com/familydollarstore/images/storefront.png\u0026quot;\\n\\t}\\t\\t\\t\\n\\t\u0026#39; Wow, looking good. The format presented here is consistent with the json format (also the type did have “json” in its name). A json object can act like a dictionary with nested dictionaries inside. It’s actually a nice format to work with once you become familiar with it (and it’s certainly much easier to program than a long series of regex commands). Although this structurally looks like a json objects, it is still a bs4 object and needs a formal programmatic conversion to json to be accessed as a json object:\narco_json = json.loads(arco_contents) type(arco_json) ## \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; print(arco_json) ## {\u0026#39;@context\u0026#39;: \u0026#39;https://schema.org\u0026#39;, \u0026#39;@type\u0026#39;: \u0026#39;Schema Business Type\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Family Dollar #9143\u0026#39;, \u0026#39;address\u0026#39;: {\u0026#39;@type\u0026#39;: \u0026#39;PostalAddress\u0026#39;, \u0026#39;streetAddress\u0026#39;: \u0026#39;157 W Grand Avenue\u0026#39;, \u0026#39;addressLocality\u0026#39;: \u0026#39;Arco\u0026#39;, \u0026#39;addressRegion\u0026#39;: \u0026#39;ID\u0026#39;, \u0026#39;postalCode\u0026#39;: \u0026#39;83213\u0026#39;, \u0026#39;addressCountry\u0026#39;: \u0026#39;US\u0026#39;}, \u0026#39;containedIn\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;branchOf\u0026#39;: {\u0026#39;name\u0026#39;: \u0026#39;Family Dollar\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://www.familydollar.com/\u0026#39;}, \u0026#39;url\u0026#39;: \u0026#39;https://locations.familydollar.com/id/arco/29143/\u0026#39;, \u0026#39;telephone\u0026#39;: \u0026#39;208-881-5738\u0026#39;, \u0026#39;image\u0026#39;: \u0026#39;//hosted.where2getit.com/familydollarstore/images/storefront.png\u0026#39;} In than content is key called ‘address’ that has the desired address information in smaller nested dictionary. This can be retrieved as thus:\narco_address = arco_json[\u0026#39;address\u0026#39;] arco_address ## {\u0026#39;@type\u0026#39;: \u0026#39;PostalAddress\u0026#39;, \u0026#39;streetAddress\u0026#39;: \u0026#39;157 W Grand Avenue\u0026#39;, \u0026#39;addressLocality\u0026#39;: \u0026#39;Arco\u0026#39;, \u0026#39;addressRegion\u0026#39;: \u0026#39;ID\u0026#39;, \u0026#39;postalCode\u0026#39;: \u0026#39;83213\u0026#39;, \u0026#39;addressCountry\u0026#39;: \u0026#39;US\u0026#39;}  Step 5: Finally, Put It All Together Iterate over the list store URLs in Idaho:\nlocs_dict = [] # initialise empty list for link in city_hrefs: locpage = requests.get(link) # request page info locsoup = BeautifulSoup(locpage.text, \u0026#39;html.parser\u0026#39;) # parse the page\u0026#39;s content locinfo = locsoup.find_all(type=\u0026quot;application/ld+json\u0026quot;) # extract specific element loccont = locinfo[1].contents[0] # get contents from the bs4 element set locjson = json.loads(loccont) # convert to json locaddr = locjson[\u0026#39;address\u0026#39;] # get address locs_dict.append(locaddr) # add address to list Do some final data organisations steps: convert to a pandas data frame, drop the unneeded columns “@type” and “country”) and check the top 5 rows to ensure that everything looks alright.\nlocs_df = df.from_records(locs_dict) locs_df.drop([\u0026#39;@type\u0026#39;, \u0026#39;addressCountry\u0026#39;], axis = 1, inplace = True) locs_df.head(n = 5) ## streetAddress addressLocality addressRegion postalCode ## 0 111 N Main Street Aberdeen ID 83210 ## 1 253 Harrison St American Falls ID 83211 ## 2 157 W Grand Avenue Arco ID 83213 ## 3 177 Main Street Ashton ID 83420 ## 4 747 N. Main St. Bellevue ID 83313 Results!!\n  They are also euphoric about these results!   Make sure to save results!! (still euphoric)\ndf.to_csv(locs_df, \u0026quot;family_dollar_ID_locations.csv\u0026quot;, sep = \u0026quot;,\u0026quot;, index = False)   A Few Words on Selenium (using Walgreens as an example)\n“Inspect Element” provides the code for what is displayed in a browser:\nWhile “View Page Source” - provides the code for what requests will obtain:\nWhen these two don’t agree, there are plugins modifying the source code - so, it should be accessed after the page has loaded in a browser. requests cannot do that, but Selenium can.\nSelenium requires a web driver to retrieve the content. It actually opens a web browser, and this page content is collected. Selenium is powerful - it can interact with loaded content in many ways (read the documentation). After getting data with Selenium, continue to use BeautifulSoup as before:\nurl = \u0026quot;https://www.walgreens.com/storelistings/storesbycity.jsp?requestType=locator\u0026amp;state=ID\u0026quot; driver = webdriver.Firefox(executable_path = \u0026#39;mypath/geckodriver.exe\u0026#39;) driver.get(url) soup_ID = BeautifulSoup(driver.page_source, \u0026#39;html.parser\u0026#39;) store_link_soup = soup_ID.find_all(class_ = \u0026#39;col-xl-4 col-lg-4 col-md-4\u0026#39;)   In Conclusion… Be Patient\nConsult the Manuals\n(these are very helpful)\n https://beautiful-soup-4.readthedocs.io/en/latest/\n https://selenium.dev/    ~ You, After Becoming a Web Scraping Master ~   Postscript: It turns out very few people nowadays have seen “Adventures in Babysitting” and get this reference in this post title. That’s okay. To summarise: while a woman babysits 3 charges for an evening, everything goes catastrophically wrong. Yet, somehow she get everyone home unharmed and have some fun adventures along the way. That felt like an apt metaphor for building these scripts. They take considerable attention, but with careful work, you can build a highly functional web scraper.\nPost Postscript:\n  Dollar Stores in America   There are many many dollar stores in America.\n ","date":1587837002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587837002,"objectID":"10cd55d0ca020163b2f8a03d46d33700","permalink":"/post/6_web_scraping_intro/","publishdate":"2020-04-25T10:50:02-07:00","relpermalink":"/post/6_web_scraping_intro/","section":"post","summary":"This is adapted from a talk I gave at Pycascades in Portland, Oregon 2020-02-09.\nAll the resources from this talk are available at my GitHub repo.\nIntroduction Here is a Great Way to Learn Python! But in truth, who actually reads these A to Z?? (spoiler: not me) Many people find these instructional books useful, but as a new Python user, I do not typically read this books front to back.","tags":["Python","web scraping"],"title":"Adventures in Babysitting: Web Scraping for the Python and HTML Novice","type":"post"},{"authors":[],"categories":null,"content":"","date":1581243600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581243600,"objectID":"40789caaf0608aba269aaa0d3dfa1fb7","permalink":"/talk/pycas2020/","publishdate":"2020-01-20T00:00:00Z","relpermalink":"/talk/pycas2020/","section":"talk","summary":"Web Scraping for Python and HTML Novices","tags":[],"title":"Adventures in Babysitting","type":"talk"},{"authors":["Julia Piaskowski"],"categories":[],"content":"Most of us think of Star Wars as the rebellion versus the empire \u0026ndash; good versus evil with key figures: the Jedi, the Sith, Darth Vader, Luke Skywalker, the Emperor, Rey, Kylo Ren. But, The downfall of the governing bodies was not Darth Vader, Luke Skywalker, Kylo Ren or Palpatine. Actually, Star Wars is a story of really bad data management practices.\nThe Republic: The Jedi maintained decent, well-organised archives in a large library and they even had paid librarians on staff. But any jedi can delete information and no record is left regarding this? For heaven’s sake, an entire planet was removed from said archives That ain’t good. It led to the Clone wars and eventual rise of the Galactic Empire. It also turns out that a person just has to look like a jedi in order to steal secured jedi archives .\nOriginal Galatic Empire:* This is where all the bad stuff starts. Initially, they had a good system for managing their vasts amount of data and an off-site backup with a very powerful firewall. However, once that firewall was breached, there was poor security inside - I mean, a dead archivist’s hand could be used to access the entire system, presumably as a root user. Their storage facility also relied on a rather inconvenient system for accessing data: a massive tower than required a remote-controlled tool for physical retrieval of data files.\nThe tool itself required manual operation and apparently the entire facility had no backup generator to maintain secruity and functioning in case of a power loss. Incredibly, there was also no search tool. Data archives had to be browsed (what???). it’s really quite astonishing that anyone ever found what they needed. Options for sending out the data were also remarkably cumbersome - a satellite dish the size of a Star Destroyer has to manually turned in the correct direction to send a file out (was the Empire capable of automating anything?). I also wonder about the file compression capabilities - seriously, is there any file so large it needs a satellite dish of that size?\nThe First Order: The First Order inherits a number of problems from the original Galatic Empire, most notably insufficient backups resulting in lost information. A map to Luke Skywalker is lost, making it nigh impossible to find him. A map to Exogol had two copies ever made, one of them on a destroyed death star sitting in the middle of raging ocean? It’s amazing these were very found. Without these maps, we have no Sith and no Jedi (maybe not such a bad thing).\nThe point is that if the Jedi had better data practices, then they might have avoided the rise of the empire. And perhaps if the Empire had better data practices, they may not undergone such a rapid downfall.\nWhat happens with good data management:  Secure data that can only be modified by authorised users Validation of data values to prevent errors Data is accessible: it can be accessed in many places, by many, in formats that are easy to use Stored data is easy to search and summarise There are sufficient backups if something goes wrong  Wait! This actually matters! This is more than a story to amuse folks, though. Poor data management is a rampant issue across academia. I have observed that most researchers do not document their data processing in sufficient detail to recreate the process. Who else has read this statement \u0026ldquo;outliers were removed\u0026rdquo; with no details into how those outliers were eliminated or the numerical values of those \u0026ldquo;outliers\u0026rdquo;?\nFrom a research standpoint this means the analytical procedure is not replicable. That means someone starting with the same raw data will not obtain the same results following the described analtical procedure \u0026ndash; because the description was inadequate. This can also mean data are unreliable. If the orginal version of data become lost as individuals clean up the data and do not document their process, then there is no way to evaluate if the cleaning was correctly done or justified. Data loss is always a risk when files are emailed around rather than shared via a more permanent data storage solution. All of these risks become amplified when an individual leaves a research program and can no longer be reached to resolve data questions.\nTools for better data management Handily, many many tools exist to assist in data management. I give a bare bones outline in this post (please read this!!). Below is an even more bare \u0026lsquo;bare bones\u0026rsquo; outline.\nOne of the best solutions is to put your data in a database. This a great tool for large quantities of data that follow a regular format and are frequently updated - such as variety testing data.\nHowever, most agricultural researchers are not so lucky to have such structured data. At the very least, maintaining research outputs in a shared and secured location is a good start. Archiving raw versions of data is paramount. This maybe never be needed, but if it is needed, there is not replacement if those data are lost. Requiring complete documentation from all research contributors on the data set generation and processing is also good. A \u0026ldquo;codebook\u0026rdquo; is another great tool. This is a file accompanying a data set that provides the name of each column, what data are present in that column, and optionally, the units of those data and the range of values expected. If a column contains contains numeric values btweeen 0 and 100, then others will know that 980 is probably an error.\nMost of the researchers I work with were trained in a different era - when data was expensive to obtain and as a result, there was considerably less data to deal with. I count myself among this group. At that time, data management tools were less relevant because we had the time to comprehensively examine a data set for errors. Furthermore, data sharing and data reuse were not widely done. But, we all now those days are long over. We are now expected to process a large quantity of data rapidly, analyse it and eventually arhive it publicaly for possible reuse by someone else. Engaging in practices that enable quality data management is essential for participation in today\u0026rsquo;s research environment.\n","date":1580579402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580579402,"objectID":"a79ed3d71a03512e029bfe8bb3531c63","permalink":"/post/5_starwars_bad_data/","publishdate":"2020-02-01T10:50:02-07:00","relpermalink":"/post/5_starwars_bad_data/","section":"post","summary":"Most of us think of Star Wars as the rebellion versus the empire \u0026ndash; good versus evil with key figures: the Jedi, the Sith, Darth Vader, Luke Skywalker, the Emperor, Rey, Kylo Ren. But, The downfall of the governing bodies was not Darth Vader, Luke Skywalker, Kylo Ren or Palpatine. Actually, Star Wars is a story of really bad data management practices.\nThe Republic: The Jedi maintained decent, well-organised archives in a large library and they even had paid librarians on staff.","tags":["data workflow"],"title":"Bad Data Management: A Star Wars Story","type":"post"},{"authors":["Julia Piaskowski","Hardner, Craig","Cai, Lichun","Zhao, Yunyang","Iezzoni, Amy","Peace, Cameron"],"categories":null,"content":"","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"3a18e64f73fbeb44bc2a5ca8aa604e21","permalink":"/publication/cherry_gebv/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/cherry_gebv/","section":"publication","summary":"Sweet cherry is consumed widely across the world and provides substantial economic benefits in regions where it is grown. While cherry breeding has been conducted in the Pacific Northwest for over half a century, little is known about the genetic architecture of important traits. We used a genome-enabled mixed model to predict the genetic performance of 505 individuals for 32 phenological, disease response and fruit quality traits evaluated in the RosBREED sweet cherry crop data set. Genome-wide predictions were estimated using a repeated measures model for phenotypic data across 3 years, incorporating additive, dominance and epistatic variance components. Genomic relationship matrices were constructed with high-density SNP data and were used to estimate relatedness and account for incomplete replication across years. High broad-sense heritabilities of 0.83, 0.77, and 0.76 were observed for days to maturity, firmness, and fruit weight, respectively. Epistatic variance exceeded 40\\% of the total genetic variance for maturing timing, firmness and powdery mildew response. Dominance variance was the largest for fruit weight and fruit size at 34\\% and 27\\%, respectively. Omission of non-additive sources of genetic variance from the genetic model resulted in inflation of narrow-sense heritability but minimally influenced prediction accuracy of genetic values in validation. Predicted genetic rankings of individuals from single-year models were inconsistent across years, likely due to incomplete sampling of the population genetic variance. Predicted breeding values and genetic values revealed many high-performing individuals for use as parents and the most promising selections to advance for cultivar release consideration, respectively. This study highlights the importance of using the appropriate genetic model for calculating breeding values to avoid inflation of expected parental contribution to genetic gain. The genomic predictions obtained will enable breeders to efficiently leverage the genetic potential of North American sweet cherry germplasm by identifying high quality individuals more rapidly than with phenotypic data alone.","tags":["genetics","plant breeding","sweet cherry"],"title":"Genomic heritability estimates in sweet cherry reveal non-additive genetic variance is relevant for industry-prioritized traits.","type":"publication"},{"authors":["Julia Piaskowski","Brown, David","Campbell, Kimberly Garland"],"categories":null,"content":"","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"9d4527f4ab39cde67d166a7917af32b1","permalink":"/publication/nir_calib/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/nir_calib/","section":"publication","summary":"Soluble stem carbohydrates are a component of drought response in wheat (Triticum aestivum L.) and other grasses. Near-infrared spectroscopy (NIR) can rapidly assay for soluble carbohydrates indirectly, but this requires a statistical model for calibration. The objectives of this study were: (i) to build a robust calibration between the NIR spectra and soluble carbohydrate concentration of ground wheat stems; and (ii) to determine whether soluble stem carbohydrates are correlated with yield rankings of drought-stricken wheat grown in the northwestern United States. Five spring wheat cultivars were grown in field trials conducted at six environments in the state of Washington varying in annual precipitation from 212 to 474 mm. Wheat stems were harvested from all plots at the onset of grain fill and assayed for NIR reflectance. Soluble stem carbohydrates were determined on a subset of the samples. The NIR data were calibrated to soluble stem carbohydrates using multiple linear regression, partial least squares regression, ridge regression with best linear unbiased prediction, random forest, least absolute shrinkage and selection operator (lasso), elastic net, and Bayesian lasso regression. Partial least squares regression provided the most accurate and reliable predictions for soluble carbohydrates. Correlations between soluble stem carbohydrates and grain yield were consistent across environments (r = 0.904 and ρ = 0.80). The effect of environment on the variation in response variables was lower for soluble carbohydrates than yield (5.93 and 71.7%, respectively) across environments. These data provide evidence that stem carbohydrates can aid in selecting cultivars with enhanced drought resilience.","tags":["drought tolerance","plant breeding","wheat"],"title":"Near-infrared calibration of soluble stem carbohydrates for predicting drought tolerance in spring wheat","type":"publication"},{"authors":["Hardner, Craig M.","Hayes, Ben J.","Kumar, Satish","Vanderzande, Stijn","Cai, Lichun","Julia Piaskowski","Quero-Garcia, José","Campoy, José Antonio","Barreneche, Teresa","Giovannini, Daniela","Liverani, Alessandro","Charlot, Gérard","Villamil-Castro, Miguel","Oraguzie, Nnadozie","Peace, Cameron P."],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"abdb30a46cb86364a619f95e71d60fcc","permalink":"/publication/sweet_cherry_maturity/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/sweet_cherry_maturity/","section":"publication","summary":"The timing of fruit maturity is an important trait in sweet cherry production and breeding. Phenotypic variation for phenology of fruit maturity in sweet cherry appears to be under strong genetic control, but that control might be complicated by phenotypic instability across environments. Although such genotype-by-environment interaction (G × E) is a common phenomenon in crop plants, knowledge about it is lacking for fruit maturity timing and other sweet cherry traits. In this study, 1673 genome-wide SNP markers were used to estimate genomic relationships among 597 weakly pedigree-connected individuals evaluated over two seasons at three locations in Europe and one location in the USA, thus sampling eight ‘environments’. The combined dataset enabled a single meta-analysis to investigate the environmental stability of genomic predictions. Linkage disequilibrium among marker loci declined rapidly with physical distance, and ordination of the relationship matrix suggested no strong structure among germplasm. The most parsimonious G × E model allowed heterogeneous genetic variance and pairwise covariances among environments. Narrow-sense genomic heritability was very high (0.60–0.83), as was accuracy of predicted breeding values ({\\textgreater}0.62). Average correlation of additive effects among environments was high (0.96) and breeding values were highly correlated across locations. Results indicated that genomic models can be used in cherry to accurately predict date of fruit maturity for untested individuals in new environments. Limited G × E for this trait indicated that phenotypes of individuals will be stable across similar environments. Equivalent analyses for other sweet cherry traits, for which multiple years of data are commonly available among breeders and cultivar testers, would be informative for predicting performance of elite selections and cultivars in new environments.","tags":["plant breeding","genetics","sweet cherry"],"title":"Prediction of Genetic Value for Sweet Cherry Fruit Maturity among Environments Using a 6K SNP Array.","type":"publication"},{"authors":["Mugabe, Deus","Coyne, Clarice J.","Julia Piaskowski","Zheng, Ping","Ma, Yu","Landry, Erik","McGee, Rebecca","Main, Doreen","Vandemark, George","Zhang, Hongbin","Abbo, Shahal"],"categories":null,"content":"","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"f680ccb00e960e9e03287b8d2b3a67ab","permalink":"/publication/chickpea_qtl_cold/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/chickpea_qtl_cold/","section":"publication","summary":"Fall-sown chickpea (Cicer arietinum L.) yields are often double those of spring-sown chickpea in regions with Mediterranean climates that have mild winters. However, winter kill can limit the productivity of fall-sown chickpea. Developing cold-tolerant chickpea would allow the expansion of the current geographic range where chickpea is grown and also improve productivity. The objective of this study was to identify the quantitative trait loci (QTL) associated with cold tolerance in chickpea. An interspecific recombinant inbred line population of 129 lines derived from a cross between ICC 4958, a cold-sensitive desi type (C. arietinum), and PI 489777, a cold-tolerant wild relative (C. reticulatum Ladiz), was used in this study. The population was phenotyped for cold tolerance in the field over four field seasons (September 2011–March 2015) and under controlled conditions two times. The population was genotyped using genotyping-by-sequencing, and an interspecific genetic linkage map consisting of 747 single nucleotide polymorphism (SNP) markers, spanning a distance of 393.7 cM, was developed. Three significant QTL were found on linkage groups (LGs) 1B, 3, and 8. The QTL on LGs 3 and 8 were consistently detected in six environments with logarithm of odds score ranges of 5.16 to 15.11 and 5.68 to 23.96, respectively. The QTL CT Ca-3.1 explained 7.15 to 34.6\\% of the phenotypic variance in all environments, whereas QTL CT Ca-8.1 explained 11.5 to 48.4\\%. The QTL-associated SNP markers may become useful for breeding with further fine mapping for increasing cold tolerance in domestic chickpea.","tags":["genetics","plant breeding","pulse crops"],"title":"Quantitative Trait Loci for Cold Tolerance in Chickpea.","type":"publication"},{"authors":null,"categories":["R language"],"content":"Sources Thomas Lumley, Github repo useRfasteR Hadley Wickham, Profiling , Advanced R\nDirk Eddelbuettel, Rcpp The Process for Improving Code: (quote from Advanced R)\n  Find the biggest bottleneck (the slowest part of your code). Try to eliminate it (you may not succeed but that’s ok). Repeat until your code is “fast enough.”   Easy peasy, right???\nSome general guidelines for speeding up R code  Use data frames less - they are expensive to create, often copied in whole when modified, and their rownames attribute can really slow things down. Be wary of using functions that copy objects in whole: c(), append(), cbind(), rbind(), or paste(). When used in loops, you can get a massive proliferation of objects in memory. Use vectorised functions:  apply, lapply, sapply, vapply, tapply, mapply rowSums, colSums, rowMeans, colMeans, cumsum, diff   Base functions are designed to handle wildly different input. Consider rewriting base functions for highly repetitive tasks. Use parallel::mclapply for parallelising functions. Consider an optimized matrix algebra library (beyond BLAS) for better performance (e.g. Apple vecLib BLAS , openBLAS ). If you work with sparse matrices, use tools for them like the package \u0026lsquo;Matrix\u0026rsquo;. For huge objects, consider storing the information in a database and accessing it with \u0026lsquo;dbplyr\u0026rsquo;. The packages \u0026lsquo;dbglm\u0026rsquo; and \u0026lsquo;tidypredict\u0026rsquo; will also do model fitting with data inside a database. Another solution for large objects are specialized formats like HDF5 or netCDF . Take advantage of the Rcpp suite of programs - not just for C/C++ programmers (e.g. RcppArmadillo::fastlm). Use an alternative implementation of R (e.g., fastR , pqR ). Check your code with benchmarking!!!  Let\u0026rsquo;s do some benchmarking! Important: don\u0026rsquo;t (re)install \u0026lsquo;compiler\u0026rsquo;; you should just be able to load it in R v3.5 and later.\npck \u0026lt;- c(\u0026quot;pryr\u0026quot;,\u0026quot;microbenchmark\u0026quot;, \u0026quot;profvis\u0026quot;, \u0026quot;compiler\u0026quot;, \u0026quot;mnormt\u0026quot;) invisible(lapply(pck, library, character.only = T))  First, Turn off the just-in-time compiler. Note that return value is what the JIT was set at previously (default = 3).\nenableJIT(0)  The microbenchmark function  for evaluating small snippets of code below is a comparison of several approaches to calculating a mean  a \u0026lt;- function() { m \u0026lt;- sample(1:100, 2) data.x \u0026lt;- lapply(m, function(x) rnorm(1e4, mean = x)) do.call(\u0026quot;cbind\u0026quot;, data.x) } some_data \u0026lt;- a() dim(some_data) microbenchmark( mean_loop = apply(some_data, 2, mean), mean_vec = colMeans(some_data), mean_manual = apply(some_data, 2, function(x) sum(x)/length(x)), mean_manual_ultra = apply(some_data, 2, function(x){ total = 0 n = 0 i = 1 while(!is.na(x[i])) { total = total + x[i] n = n+1 i = i+1 } total/n }) )  Prevent multiple dispatch:  the function mean() is meant to handle several different types of data specifying the method (thus implying a certain type of input) can speed up the process for small data sets the function mean() calls a different function depending on the object specified:  methods(mean) x1 \u0026lt;- list(e2 = runif(1e2), e4 = runif(1e4), e6 = runif(1e6)) lapply(x1, function(x) microbenchmark( mean(x), mean.default(x) ) )  I suspect the improvement in speed for smaller objects but larger objects is related to big O notation \u0026ndash; these smaller objects are impacted by constants\nOther approaches for finding source code:\n# tracking package type, etc otype(mean) ftype(mean) showMethods(mean) #for S4 methods(mean)  The function methods() does not always work, but there are alternatives:\nmethods(var) getAnywhere(var)  Find the bottlenecks with Rprof()  writes stack calls to disk along with memory usage and vector duplication you create a .prof file to do this and then close it when done with profiling  Rprof(\u0026quot;permute.prof\u0026quot;, memory.profiling = T) sigma.mv \u0026lt;- diag(1, nrow = 5, ncol = 5) sigma.mv[upper.tri(sigma.mv)] = 0.5 sigma.mv[lower.tri(sigma.mv)] = 0.5 mvn.data \u0026lt;- rmnorm(1e3, mean = rep(0, 5), varcov = sigma.mv) colnames(mvn.data) \u0026lt;- c(paste0(\u0026quot;x\u0026quot;,1:5)) kmeans.Ftest \u0026lt;- function(kmean_obj) { df.1 = length(kmean_obj$size) - 1 df.2 = length(kmean_obj$cluster) - length(kmean_obj$size) betw_ms \u0026lt;- kmean_obj$tot.withinss/df.1 with_ms \u0026lt;- kmean_obj$betweenss/df.2 fratio = betw_ms/with_ms pval \u0026lt;- pf(fratio, df1 = df.2, df2 = df.1, lower.tail = F) stuff = c(fratio, df.1, df.2, pval) names(stuff) \u0026lt;- c('F-ratio', 'df 1','df 2', 'p-value') return(stuff) } kmeans.optimiz \u0026lt;- lapply(2:10, function(x) { results = kmeans(mvn.data, centers = x, nstart = 15, algorithm = \u0026quot;MacQueen\u0026quot;, iter.max = 50) kmeans.Ftest(results) }) kmeans.final \u0026lt;- do.call(\u0026quot;rbind\u0026quot;, kmeans.optimiz) Rprof(NULL) summaryRprof(\u0026quot;permute.prof\u0026quot;) #, memory = \u0026quot;both\u0026quot;)  Use profvis to visualize performance  nice graphical output native in RStudio (there is extensive documentation ) you can view output in browser  p \u0026lt;- profvis({ mean_loop = apply(some_data, 2, mean) mean_vec = colMeans(some_data) mean_manual = apply(some_data, 2, function(x) sum(x)/length(x)) mean_manual_ultra = apply(some_data, 2, function(x){ total = 0 n = 0 i = 1 while(!is.na(x[i])) { total = total + x[i] n = n+1 i = i+1 } total/n }) }) htmlwidgets::saveWidget(p, \u0026quot;profile.html\u0026quot;) browseURL(\u0026quot;profile.html\u0026quot;)  Explore source code How to access function source code (if you didn\u0026rsquo;t write the function yourself)\n Type the function name (without parentheses): eigen Find namespace and methods associated: methods(\u0026quot;princomp\u0026quot;); getAnywhere(\u0026quot;princomp.default\u0026quot;) Use pryr::show_c_source() to search for C code on GitHub (which may or may not be helpful) Download the entire package and explore the code  svd La.svd try(show_c_source(.Internal(La_svd(x)))) show_c_source(.Internal(mean(x))) download.packages(\u0026quot;broman\u0026quot;, repos = \u0026quot;https://cloud.r-project.org\u0026quot;, destdir = getwd(),type = \u0026quot;source\u0026quot;)  Trace objects copied  use tracemem() to track particular objects  a \u0026lt;- letters[sample(10)] tracemem(a) a[1] \u0026lt;- \u0026quot;Z\u0026quot; b \u0026lt;- a[1:5] # not copied  Memory  How much memory is being used? Note that R requests memory from your computer in big chunks then manages it itself.  mem_used() object.size(x1) #base object_size(x1) #pryr compare_size(x1) #between base R and pryr  Read the documentation for pryr functions for more useful functions.\nNow that we are all done, turn the JIT compiler back on:\nenableJIT(3)  More Tips from Advanced R These are designed to reduce internal checks\n read.csv(): specify known column types with colClasses. factor(): specify known levels with levels. cut(): don\u0026rsquo;t generate labels with labels = FALSE if you don\u0026rsquo;t need them, or, even better, use findInterval(). unlist(x, use.names = FALSE) is much faster than unlist(x). interaction(): if you only need combinations that exist in the data, use drop = TRUE.  Remember!  xckd comic ","date":1537379402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537379402,"objectID":"3439056fa720d0af8b7049a0089af11b","permalink":"/post/4_code_profiling/","publishdate":"2018-09-19T10:50:02-07:00","relpermalink":"/post/4_code_profiling/","section":"post","summary":"Sources Thomas Lumley, Github repo useRfasteR Hadley Wickham, Profiling , Advanced R\nDirk Eddelbuettel, Rcpp The Process for Improving Code: (quote from Advanced R)\n  Find the biggest bottleneck (the slowest part of your code). Try to eliminate it (you may not succeed but that’s ok). Repeat until your code is “fast enough.”   Easy peasy, right???\nSome general guidelines for speeding up R code  Use data frames less - they are expensive to create, often copied in whole when modified, and their rownames attribute can really slow things down.","tags":["profvis","profiling","Jupyter"],"title":"Faster R Scripts through Code Profiling","type":"post"},{"authors":null,"categories":["R language"],"content":"I was struggling with the data set. The first 50 rows were summary stats (\u0026hellip;I think), and then the actual data started. The file was over 250 columns wide, composed largely of phenotypic traits. Perhaps one third of the columns were unique variables gathered over several years, but, the variable names were inconsistent across years. Actually, there was no information on the variables, what they were measuring, and what scale they were on. The closest hint was a pattern of color coding applied to groups of columns: yellow, green, blue. There appeared to be partial duplication and/or combination of of some columns, but we could not reconstruct exactly what had happened to the data. No one could recall the full history of the file, what that color coding meant, who had pre-processed data, or what they had done. We did know that the raw data were long lost.\nThis was the not the first nor the last messy data set with an unknown history that complicated my capacity to organize and analyse the information.\nOther problems I encountered with poorly managed data sets:\n Date sets with outliers removed based on individual\u0026rsquo;s researchers assessment on what looked weird or not. An Excel file several thousand lines with miscellaneous comments added every so often indicating a data point may be suspect. An Excel file several sheets wide of the same information across different years - yet the column names did not match, and some sheets included summary stats for groups of data, using empty rows to differentiate groups (again, I think this was what going on\u0026hellip;).  There\u0026rsquo;s no way to sugarcoat it: management of raw experimental data and the analysis workflow is largely abysmal in the agricultural sciences, my academic home for 14 years. The process of cleaning, rearranging, filtering and preparing experimental data for downstream analyses is poorly described, if at all, in most papers. As many of know, much of data conditioning and preparation is done by students and technicians with little formal guidance or documentation of their work.\nSince data sets can and should live on for decades - as part of long-term agriculture research, for meta-analyses or other uses, understanding how the raw data were parsed, cleaned and interpreted is important. Assumptions that made sense in the initial analysis may need re-evaluation later. Intermediate files can turn out to be useful. Organization of an analysis workflow can help researchers conduct their research in a regimented and reproducible way.\nHere are a few concrete recommendations for improving workflow:\n Archive the raw, unaltered version of data set. Create a consistent directory structure for each project - that way you are storing things consistently and can find them. Really, this makes things easier for everyone. For a data set, each column should be include only type of data corresponding to the column name. These data can be on different scales. For example, if one column indicates the type of variable (e.g. pH, moisture content, nitrate concentration), another columns could indicate the actual value for that trait and observation. It is less confusing if a column only contain a similar type of data (e.g raw OR transformed OR mean value), not data and summary stats together. Avoid use of formatting that cannot be interpreted as plain text. By this, I mean do not insert comments in Microsoft office files, or use text formatting tools (bolding, color coding) to differentiate observations. This information can easily become lost to time and does not translate well to other software, especially command-line software. Comments, categorical variables, et cetera should each have their own column. Summary stats of a data set belong in their own table or file. Use a non-proprietary format like CSV or TXT. Avoid creating data that relies on a particular order of the observations (for instance, every 5th row is the mean of the previous 4 rows). One sorting step can ruin this. If a special order is needed, create a column which implements this. Make a \u0026ldquo;codebook\u0026rdquo; documenting what each variable means, what scale it is on, expected levels or range of data. This can be one sheet of information - very brief, but handy. Giving variables short, descriptive names is also helpful. Document what happens to a data set - and make that information available to others. I prefer to use a command line tool like R, but a text file could do. This is to ensure your work can be duplicated. I realize programming is not for everyone, but using a command-line program like R, Python, Julia, SAS, or even bash, is one of the best ways to document how information is pre-processed. Changes made through point-and-click interfaces like Excel are not reproducible because they are almost always incompletely documented or not documented at all. In addition, manual manipulation in a spreadsheet program also results in user-generated errors: misspellings, misclassifications, cut-and-paste errors and so on. Accountability and follow up: attach people\u0026rsquo;s names to their data and/or analysis. Back up your data: across at least 2 locations over the life of your project. Version control make it possible for your work to be undone by accessing earlier versions. Git is one such tool, but saving intermediate file versions (often!) can also help. By providing descriptive comments at each new commit (a version), that can help downstream researchers sift through previous versions of the data. Also, some cloud storage providers (like Dropbox) offer options for data recovery. Coordinate with project collaborators regarding workflow expectations and processes  The core reason I am interested in these issues is to improve reproducibility. As I dealt with the aforementioned issues with data, I could not help but wonder if I was making the right choices with the data, and wish that I had more information on the background of these legacy data sets. This is particularly important as meta-studies continue to gain in popularity. Unifying data sets and leveraging their combined power is one of the priorities for agricultural productivity identified by the National Academy of Sciences in their recent report, Science Breakthroughs to Advance Food and Agricultural Research by 2030 .\nFor more information, see this excellent article by Kara Woo and Karl Broman - it\u0026rsquo;s short and gives easy-to-follow best practices when working with spreadsheets.\nScott Long at the University of Indiana put together an informative set of slides delving further into the principles of data workflow. Patrick Schloss, of the bioinformatics program mothur, recently published an article delving into the problem of reproducibility in microbiome work and how to fix it. Among many things, he recommended implementing robust workflow practices.\n","date":1533664202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533664202,"objectID":"bad9f1e027076d371ac530f0b08357da","permalink":"/post/3_workflow_basics/","publishdate":"2018-08-07T10:50:02-07:00","relpermalink":"/post/3_workflow_basics/","section":"post","summary":"I was struggling with the data set. The first 50 rows were summary stats (\u0026hellip;I think), and then the actual data started. The file was over 250 columns wide, composed largely of phenotypic traits. Perhaps one third of the columns were unique variables gathered over several years, but, the variable names were inconsistent across years. Actually, there was no information on the variables, what they were measuring, and what scale they were on.","tags":["data workflow","reproducibility"],"title":"Best Practices for Data Workflows in Ag Science","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R language"],"content":"Top Lessons from R users conference held July 10 - 13 in Brisbane, AUS With 900 registrants and dozens of talks, there is much to report ( videos of most talks provided by R Consortium YouTube channel) I\u0026rsquo;ll skip loads of it and just focus on the top 10 cool stuff.\n  The hex wall was just straight up cool. Here\u0026rsquo;s the code for that .\n  I participated in a half day workshop on Rcpp ( here and here ). This continues to be a very popular suite of packages to help users increase the speed and efficiency of their programs. I say \u0026ldquo;suite\u0026rdquo; because in addition to 3 major functions provided by Rcpp, a number of accessory packages have been written to extend its functionality. Several of these accessory packages essentially are providing templates for invoking C++ commands without having to actually know C. Dirk Eddelbuttel introduced the 3 major functions for extending R with C/C++:\n   evalCpp: for evaluating ad hoc anonymous expressions cppFunction: standard function written in C/C++ and invoked in a program sourceCpp: source C/C++ objects \u0026amp; functions from external file These functions must be compiled each time a fresh R session is started. Building an Rcpp package avoids this and is easy to do:  Rcpp.package.skeleton(\u0026quot;mypackage\u0026quot;)   The other useful workshop I participated in was methods for speeding up R, delivered by Thomas Lumley, formerly of UW and now of U of Auckland. He focused on a few major recommendations:    Vectorise whenever possible. This also implies that you know your vectorised functions:\n lapply/apply/sapply/vapply/tapply rowMeans, rowSums colMeans, colSums Consider parallelised functions: mclapply    Base functions are often designed to handle wildly different input. It may make sense in some instances to rewrite functions, making assumptions (e.g. about the input data) that fit your circumstances.\n  Data frames should be avoided if possible - they are expensive to create and often copied in whole when modified.\n  Using an optimized matrix algebra library (not LAPACK) may be worth the time to install that. These libraries have optimized how data is accessed from disk.\n  Some rec\u0026rsquo;s on how to handle large objects that reasonably don\u0026rsquo;t fit in memory:\n buy a computer with more memory (not always a reasonable option) put your data in database and access with tools like dbplyr. One recommendation is MonetDBLite, a column-optimized database good for scientific applications for that reason. Use special file formats for bigger data sets: HDF5, GitLFS    But, first and foremost, he emphasized the use of profiling tools and benchmarking functions. find out where the bottleneck in your program actually are! And be careful to not waste too much time on optimizing code - time saved may not be worth time invested. R is automatically running a JIT (just in time) compiler that makes code run faster each time it is called. It makes sense to turn this off while doing benchmarking:\n  enableJIT(0) # to turn back on: enableJIT(1)  Course notes available on his github repo UseRfasteR .\n  Excellent keynotes all-around. Steph da Silva emphasized the importance of community in open-source community - what that can look like, how to contribute how your contribution helps develop camaraderie to support sharing of code and development of analysts.\n   Roger Peng discussed development of R as it strives to fit both programmers and regular ole scientists analyzing their data. He summarized his own talk quite well in this essay. He described the rise of \u0026ldquo;worse is better\u0026rdquo;, that is, the simplying of options in R to make it easier for new users to learn. The tidyverse is considered such an example. However, as a long-time user, I find dplyr\u0026rsquo;s group-and-summarise options (among others) to be incredibly handy. And I will never go back to using reshape() now that we have gather() and spread(). Even reshape2() is a massive improvement in usability!\n  Jenny Bryan\u0026rsquo;s talk on \u0026ldquo;Code Smells and Feels\u0026rdquo; provided a great introduction to how to identify and fix poorly written code. According to Wikipedia , \u0026ldquo;Code smells are usually not bugs; they are not technically incorrect and do not prevent the program from functioning. Instead, they indicate weaknesses in design that may slow down development or increase the risk of bugs or failures in the future.\u0026rdquo; Based on the book, Refactoring: Improving the Design of Existing Code, she gave a few simple directives to create simple code that is easy to understand, debug, and maintain. Hooray!\n   Hoist entry or exit conditionals to the top of functions Use functions as much as possible Avoid overly nested code (e.g. a long cascade of nested \u0026ldquo;if\u0026rdquo; statements) Not every \u0026ldquo;if\u0026rdquo; needs an else Consider implementing true object-oriented programming when working with classes Don\u0026rsquo;t\u0026rsquo; be afraid to write short helper functions There was also a brief discussion on how to tell a person their code smells bad (\u0026ldquo;that\u0026rsquo;s what we have funny names like \u0026lsquo;excessive use of literals\u0026rsquo;\u0026quot;).    Excellent set of talks on data handling and workflow. In particular, a talk on the website builder for documenting data analysis called \u0026ldquo;workflowr\u0026rdquo; . Discussion on consistent workflows , recommended conventions for naming variables, creating consistent set of directories for each project, using version controls. Obvious to a data scientist, but rarely used by regular scientist. This resonates strongly with me due to ongoing issues with data management in the agricultural sciences.\n   Excellent talk on precision in R . Use .Machine to find out more, but basically R is only good to 16 decimal points. Subtraction of one number similar to it can result in cancellation where the result essentially slides to zero, losing precision. Ways to avoid this include working in log space, perhaps adding a zero if needed: log(x + 1), -log(1 - exp(x)). Also, there is the Rmpfr package .\n   Glue package - for \u0026ldquo;gluing\u0026rdquo; strings to data, like string interpolation in bash (\u0026ldquo;$\u0026rdquo;) or python f\u0026rdquo;{\u0026hellip;}\u0026quot;. R does have sprintf() with with identical functionality to C\u0026rsquo;s printf(), but glue makes this easier.\n   Fun talk on vwline package for creating variable-width lines, like in Menaurd\u0026rsquo;s famous plot of Napolean\u0026rsquo;s march through Russia.\n  ","date":1531936202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531936202,"objectID":"fbdec96be334aba28f19e278bcd34dc9","permalink":"/post/2_user_2018/","publishdate":"2018-07-18T10:50:02-07:00","relpermalink":"/post/2_user_2018/","section":"post","summary":"Top Lessons from R users conference held July 10 - 13 in Brisbane, AUS With 900 registrants and dozens of talks, there is much to report ( videos of most talks provided by R Consortium YouTube channel) I\u0026rsquo;ll skip loads of it and just focus on the top 10 cool stuff.\n  The hex wall was just straight up cool. Here\u0026rsquo;s the code for that .\n  I participated in a half day workshop on Rcpp ( here and here ).","tags":["data workflow","Rcpp"],"title":"What I learned at the UseR! conference","type":"post"},{"authors":["Vanderzande, Stijn","Julia Piaskowski","Luo, F","Edge-Garza, D","Peace, C"],"categories":null,"content":"","date":1524355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524355200,"objectID":"ef601594933419ed7984be99e8831897","permalink":"/publication/dna_testing/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/dna_testing/","section":"publication","summary":"DNA-informed breeding, the integration of DNA-based genetic information into plant breeding programs, can enhance efficiency, accuracy, creativity, and pace of new cultivar development. Most genetic knowledge on key traits for plant breeding has been obtained through QTL analyses. Despite an explosion in QTL discoveries for horticultural crops, very few of those discoveries have been translated into tools for horticultural crop breeding. An example of such tools with direct application in crop genetic improvement are trait-predictive DNA tests. The translation of a promising QTL to a trait-predictive DNA test has five steps: (1) choose target QTL; (2) design assay to target locus; (3) assay individuals; (4) trace inheritance; and (5) disseminate DNA test details. Key information to convey to end users about a DNA test are the crop and trait(s) addressed, targeted trait locus or loci, and marker type used; trait heritability and genotypic variance explained by the DNA test; allele effects, frequencies, and germplasm distributions; and technical details for running the test. This paper provides instructions for translating promising QTLs into breeder-friendly, trait-predictive DNA tests, based on our experience with tree fruit. Our intent is to accelerate development of trait-predictive DNA tests and establish a standard framework for reporting them. As scientific understanding of genetic factors controlling breeding-relevant traits continues to expand, systematic and increased DNA test development should help bridge the chasm between academic research and breeding application.","tags":["genetics","plant breeding"],"title":"Crossing the finish line: how to develop diagnostic DNA tests as breeding tools after QTL discovery","type":"publication"},{"authors":["Julia Piaskowski"],"categories":["workflow"],"content":"Basic Info I came back from PAG (Plant \u0026amp; Animal Genome - a huge ag genetics conference held in San Diego each January) all fired about setting up a virtual machine (VM) to facilitate collaboration. The hope what this would enable us to share files and scripts better (without the aid of countless dropboxes everyone forgets about), and improve overall collaboration.\nI had considered this before - enterprise deployment of RStudio server or Jupyter hub, but our program doesn’t have dedicated IT support to do this, and frankly, it all seemed rather intimidating to a plant geneticist like me. But this would be useful. I had once run my laptop on dual boot with Ubuntu and Windows. I could handle this, couldn’t I? All I want is a VM that everyone can access via remote desktop for their analytical needs. Fortunately, I had no idea how difficult this would be, or I might not have entered the lion’s den. So, here are fruits of my labor in the hopes it will ease the process for future users. This guide was developed specific for Washington State University\u0026rsquo;s College of Agriculture, Human and Natural Resource Sciences (CAHNRS).\nStep 1: Request a virtual machine. I used the CAHNRS webform . I went with CentOS 7 - a Linux distribution that had been recommended to me as “full featured” (this remains to be seen). This step may take a few weeks.\nIt helps to know exactly what you want to do so CAHNRS IT can properly set up the VM and “rules” (exceptions to their standing rules) to allow you to do the work you want. Running RStudio server means having at least one port open. The RStudio Server default port is 8787 (although than can be easily changed). Remote desktop needs port 3350 open. Jupyter Hub will have its own conventions - all described in their documentation. I recommend you consult the documentation for what you want before ordering a VM.\nStep 2: Download the WSU VPN client . Whoever is the admin will need have the WSU SSLVPN to tunnel in through a secure connection in order to do any work remotely. This is the probably the easiest step of the entire process. Who doesn’t love point-and-click installations?\nStep 3: Download software for SSH. Initially, you will need an SSH program to connect through secure, encrypted tunnel. Putty is a popular option. I like secure shell, which I found here . This is actually a rather convenient way to manage the VM, so its utility will likely stretch past the setup phase.\nGeneral Linux notes: This is CentOS, an alternative distribution to its more popular relative, Ubuntu. While most commands are the same across Linux distributions, some things are a little different. FYI, Centos is similar to .rhel and Red Hat if you are combing forums for help.\nCheck out these standards for Linux filesystems . It will help you understand how the machine is organized. Don’t spend too much time here; it’s more of a reference than pleasure reading.\n The Centos documentation can be helpful, as well.\nMiscellaneous useful commands: Most of the command listed here for VM setup require root privileges, which can be accomplished with “sudo”. You’ll find you need to be root 98% of time during VM setup, so do this:\nsudo su\nThis changes the console from: $  to: # \nUse Ctrl-D to leave root. And be careful when you are root!\nPrint current working directory:\npwd\nChange the working directory to go up a level:\ncd .. \nAbsolute path (will work everywhere):\n$ cd /\u0026lt;path\u0026gt;\nRelative path (only sees child directories):\ncd \u0026lt;path\u0026gt;\nThis produces a long list of all files and directories in your current working directory:\nll -a\nTo see options associated with a particular command:\n\u0026lt;command\u0026gt; --help\nTo remove a single file:\nrm \u0026lt;your_file.txt\u0026gt;\nTo remove an entire directory:\n‘r’ does it recursively\n‘f’ will do it without asking again (it skips the step that is essentially asking “are you sure you want to delete this????”)\nrm -rf \u0026lt;your_directory\u0026gt;\nTo install a piece of software for all users:\nsudo yum install \u0026lt;library\u0026gt;\nTo view files:\nmore \u0026lt;file\u0026gt;\nTo edit files. This command requires sudo for most of the files you will need to look at for VM setup.\nnano \u0026lt;file\u0026gt;\nThere’s also the vim editor, but use it your own risk. Here is the best description I have ever read of it:\n “If you want an outstanding free text editor and don\u0026rsquo;t mind a seemingly vertical learning curve plus long days of pain and suffering while all your neural pathways are rewired, try Vim.”\n vi \u0026lt;file\u0026gt; \nTo escape vim:\n :q \nMake a new directory:\nmkdir \u0026lt;dir_name\u0026gt;\nSetting your VM to the correct timezone (Pacific in this case). It is helpful to have the correct time when you are inspecting the logs during troubleshooting:\ntimedatectl set-timezone 'America/Los_Angeles'  Managing Users It is important to think thru how to organize the users into groups and create folders with shared permissions in order to enable collaboration.\nBelow are some common commands. Note that they all require root or sudo access. Centos provides excellent documentation on this.\nAdd a new user (the second option adds a new user to an existing group):\nuseradd \u0026lt;username\u0026gt;\nuseradd \u0026lt;username\u0026gt; -g \u0026lt;groupname\u0026gt;\nSet user password:\npasswd \u0026lt;username\u0026gt;\nAdd a group:\ngroupadd \u0026lt;groupname\u0026gt;\nAdd an existing user to an existing group:\nsudo usermod -a -G \u0026lt;groupname\u0026gt; \u0026lt;username\u0026gt;\nEnable file ownership by a group:\nchown :\u0026lt;groupname\u0026gt; \u0026lt;path/to/directory\u0026gt;\nEnable file ownership by a user:\nchown :\u0026lt;username\u0026gt; \u0026lt;path/to/directory\u0026gt;\nDelete a user:\nuserdel \u0026lt;username\u0026gt;\nLink directories:\nln -s \u0026lt;path/to/file\u0026gt; \u0026lt;path/to/link\u0026gt;\nIf you want to run remote desktop: There are other desktop environments, but Gnome is a popular one, so why not?\nSee what is available:\nyum group list  Install Gnome desktop environment:\nsudo yum install gnome sudo yum install gnome desktop environment  Check that it is running\nps -aux | grep gnome  Install and configure the EPEL repository:\nsudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  Add the nux repository:\nsudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpm  Install xrdp:\nsudo yum -y install xrdp tigervnc*  You’ll need to start xrdp and xrdp-sesman (“session manager”) as a service (so they are always running) and enable them both to autostart:\nsudo systemctl start xrdp sudo systemctl enable xrdp sudo systemctl start xrdp-sesman sudo systemctl enable xrdp-sesman  And check:\nsudo systemctl status xrdp sudo systemctl status xrdp-sesman  These should indicate that xrdp and xrdp-sesman are active.\nOpen a port to allow remote desktop connections:\nsudo firewall-cmd --permanent --zone=public --add-port=3389/tcp sudo firewall-cmd --reload  Configure SELinux:\nsudo chcon --type=bin_t /usr/sbin/xrdp sudo chcon --type=bin_t /usr/sbin/xrdp-sesman  And check that the port is open:\nnetstat -plan | grep xrdp  Next, vnc has to be started. It is tied to a user, so get out of root if you are there (and don’t use sudo).\nSet a password:\nvncpasswd -\u0026lt;user\u0026gt;  It will return a prompt, asking you for a password. This is what you will use for remote desktop. You can definitely skip the step of having a read-only password by answering “n” for no. (It’s unclear to me why anyone would want a password for that as opposed to whole new user account with read-only permissions, but what do I know?).\nNext, start the server. Try to only do this once.\nvnc server  If you end up with multiple vnc server instances running on just ONE user account, kill the extra processes and delete their log files. Here are the steps:\nFirst, find the process ID’s:\nps aux | grep vnc  The process ID\u0026rsquo;s (pid) are located in the second column from the left. Kill the extra vnc pid\u0026rsquo;s:\nkill -9 \u0026lt;pid\u0026gt;  Find the associated files and delete them. I found there here:\ncd /var/log  They will have names like \u0026ldquo;Xorg.9.log\u0026rdquo;.\nTroubleshooting Remote Desktop If you run into trouble, try disabling SELinux:\nsudo setenforce 0  Check:\ngetenforce  (it should say “permissive” NOT “enforcing)\nCheck for problems (as root):\ncd /var/logs more xrdp.log more xrdp-sesman.log  These are cleared rapidly and folded into the xrdp .tar files, so do not be alarmed if the log files are empty. If you really want to see something in the xrdp logs, stop and restart xrdp.\nThe most useful file is the messages file, but it’s very long, so just show the end:\nsudo tail -55 messages  If you really want to see what\u0026rsquo;s going on, watch the file in real time:\ntail -f messages\nCtlr-C to leave\nThe firewalld file in the /var/log/ directory is stuffed with known, but currently unresolved bugs so it’s not very useful for diagnosing problems at this time.\nThere\u0026rsquo;s a number of remote desktop applications which can be used to access the VM. Windows RDP is easy to use and secure.\nRun R in remote desktop Start by installing R. WSU would not let me access the internet directly from the remote desktop internet browser, but I could through the SSH. So here are command line instructions:\nsudo yum install epel-release sudo yum update sudo yum install R -y  Next, install RStudio. Get current link from the RStudio website . The link below worked March 19, 2018:\nwget https://download1.rstudio.org/rstudio-1.1.442-x86_6 sudo rpm -Uvh rstudio-1.1.442-x86_64.rpm  Other It\u0026rsquo;s also helpful to have an office suite\nsudo yum install libreoffice  Okay, all done! Go and solve the world’s problems!\n","date":1521395402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521395402,"objectID":"127916948b2e3581dbec55298e2779da","permalink":"/post/1_vm_wsu/","publishdate":"2018-03-18T10:50:02-07:00","relpermalink":"/post/1_vm_wsu/","section":"post","summary":"Basic Info I came back from PAG (Plant \u0026amp; Animal Genome - a huge ag genetics conference held in San Diego each January) all fired about setting up a virtual machine (VM) to facilitate collaboration. The hope what this would enable us to share files and scripts better (without the aid of countless dropboxes everyone forgets about), and improve overall collaboration.\nI had considered this before - enterprise deployment of RStudio server or Jupyter hub, but our program doesn’t have dedicated IT support to do this, and frankly, it all seemed rather intimidating to a plant geneticist like me.","tags":["virtual machine","Jupyter"],"title":"How to Set Up a Virtual Machine for Data Analysis If You Are an IT Admin Noob","type":"post"},{"authors":["Julia Piaskowski","Murphy, Kevin","Kisha, Theodore","Jones, Stephen"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"df7178d064c2b324c94dba277efd7e18","permalink":"/publication/perennial_wheat/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/perennial_wheat/","section":"publication","summary":"Perennial wheat has been proposed to alleviate long standing issues with soil erosion in annual cropping systems, while supporting rural communities and providing grain farmers with a marketable climate-resilient crop. The Washington State University perennial wheat breeding program has created several hundred interspecific progeny by crossing several different cultivars of winter wheat (Triticum aestivum L.) with Thinopyrum species and × Agrotriticum spp. Prior to the chromosome composition of these wheat-wheatgrass derivatives was not characterized, limiting their utility as stable breeding germplasm. We determined the mitotic chromosome number and species origin of chromosomes for eight breeding lines, and estimated their relatedness and population structure using AFLPs. Additionally, self-pollination and outcrossing rates were estimated for these breeding lines to gain an understanding of perennial wheat’s reproductive strategy. We intercrossed the lines with each other to produce 20 families and then measured the level of chromosome pairing during meiosis I in the F1 progeny. The lines contained between 44 and 64 chromosomes, of which eight to 16 were from Th. ponticum. Our analysis of molecular diversity indicated greater genetic diversity within, rather than across, breeding lines (88 and 12\\%, respectively). The outcrossing rate was estimated at 16\\%. Understanding chromosome number and origin is necessary for developing a population of breeding lines that can be used as parents. Our results suggest that the perennial wheat breeding lines act as a single diverse population that can be improved using breeding strategies for inbred and outcrossing crops.","tags":["wheat","population genetics"],"title":"Perennial wheat lines have highly admixed population structure and elevated rates of outcrossing","type":"publication"},{"authors":["Julia Piaskowski","Weddell, B","Fuerst, E.P.","Roberts, D","Carpenter-Boggs, L"],"categories":null,"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"54ebdbe9b9f22673355c0e33e0e4d760","permalink":"/publication/networks/","publishdate":"2013-12-01T00:00:00Z","relpermalink":"/publication/networks/","section":"publication","summary":"Extension can play a valuable role by bringing together those who are pioneering innovative practices. We planned, built, and evaluated an Extension symposium on dryland organic agriculture. Post-symposium evaluations indicated that this process disseminated regionally relevant information; fostered networks among producers, researchers, and the organic processing and feed industries; enhanced trust among stakeholders; and increased interest in expanding organic production. Ninety-five percent of respondents indicated that they established new business relationships within 6 months of the symposium. A unique aspect of our project was the enhancement of social capital between geographically separated rural localities.","tags":["extension","wheat","dryland agriculture"],"title":"Building supportive networks among agricultural innovators through a symposium on dryland organic farming","type":"publication"}]