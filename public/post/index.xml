<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Julia Piaskowski</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Feb 2020 10:50:02 -0700</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Star Wars and Bad Data Management</title>
      <link>/post/starwars_bad_data/</link>
      <pubDate>Sat, 01 Feb 2020 10:50:02 -0700</pubDate>
      <guid>/post/starwars_bad_data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Faster R Scripts through Code Profiling</title>
      <link>/post/code_profiling/</link>
      <pubDate>Wed, 19 Sep 2018 10:50:02 -0700</pubDate>
      <guid>/post/code_profiling/</guid>
      <description>&lt;h5 id=&#34;sources&#34;&gt;Sources&lt;/h5&gt;
&lt;p&gt;Thomas Lumley, Github repo 
&lt;a href=&#34;https://github.com/tslumley/useRfasteR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;useRfasteR&lt;/a&gt;
&lt;br&gt;
Hadley Wickham, 
&lt;a href=&#34;http://adv-r.had.co.nz/Profiling.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Profiling&lt;/a&gt;
, &lt;em&gt;Advanced R&lt;/em&gt;&lt;br&gt;
Dirk Eddelbuettel, 
&lt;a href=&#34;http://www.rcpp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rcpp&lt;/a&gt;
&lt;/p&gt;
&lt;h4 id=&#34;the-process-for-improving-code&#34;&gt;The Process for Improving Code:&lt;/h4&gt;
&lt;p&gt;(quote from &lt;em&gt;Advanced R&lt;/em&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Find the biggest bottleneck (the slowest part of your code).&lt;/li&gt;
&lt;li&gt;Try to eliminate it (you may not succeed but that’s ok).&lt;/li&gt;
&lt;li&gt;Repeat until your code is “fast enough.”&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Easy peasy, right???&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./owl.png&#34; alt=&#34;owl&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;some-general-guidelines-for-speeding-up-r-code&#34;&gt;Some general guidelines for speeding up R code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Use data frames less - they are expensive to create, often copied in whole when modified, and their rownames attribute can really slow things down.&lt;/li&gt;
&lt;li&gt;Be wary of using functions that copy objects in whole: c(), append(), cbind(), rbind(), or paste(). When used in loops, you can get a massive proliferation of objects in memory.&lt;/li&gt;
&lt;li&gt;Use vectorised functions:
&lt;ul&gt;
&lt;li&gt;apply, lapply, sapply, vapply, tapply, mapply&lt;/li&gt;
&lt;li&gt;rowSums, colSums, rowMeans, colMeans, cumsum, diff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Base functions are designed to handle wildly different input. Consider rewriting base functions for highly repetitive tasks.&lt;/li&gt;
&lt;li&gt;Use parallel::mclapply for parallelising functions.&lt;/li&gt;
&lt;li&gt;Consider an optimized matrix algebra library (beyond BLAS) for better performance (e.g. 
&lt;a href=&#34;https://developer.apple.com/documentation/accelerate/blas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple vecLib BLAS&lt;/a&gt;
, 
&lt;a href=&#34;https://www.openblas.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openBLAS&lt;/a&gt;
).&lt;/li&gt;
&lt;li&gt;If you work with sparse matrices, use tools for them like the package &amp;lsquo;Matrix&amp;rsquo;.&lt;/li&gt;
&lt;li&gt;For huge objects, consider storing the information in a database and accessing it with &amp;lsquo;dbplyr&amp;rsquo;. The packages &amp;lsquo;dbglm&amp;rsquo; and &amp;lsquo;tidypredict&amp;rsquo; will also do model fitting with data inside a database.&lt;/li&gt;
&lt;li&gt;Another solution for large objects are specialized formats like 
&lt;a href=&#34;https://portal.hdfgroup.org/display/support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HDF5&lt;/a&gt;
 or 
&lt;a href=&#34;https://www.unidata.ucar.edu/software/netcdf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;netCDF&lt;/a&gt;
.&lt;/li&gt;
&lt;li&gt;Take advantage of the Rcpp suite of programs - not just for C/C++ programmers (e.g. RcppArmadillo::fastlm).&lt;/li&gt;
&lt;li&gt;Use an alternative implementation of R (e.g., 
&lt;a href=&#34;https://github.com/oracle/fastr]&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastR&lt;/a&gt;
, 
&lt;a href=&#34;http://www.pqr-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pqR&lt;/a&gt;
).&lt;/li&gt;
&lt;li&gt;Check your code with benchmarking!!!&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;lets-do-some-benchmarking&#34;&gt;Let&#39;s do some benchmarking!&lt;/h4&gt;
&lt;p&gt;Important: don&#39;t (re)install &amp;lsquo;compiler&amp;rsquo;; you should just be able to load it in R v3.5 and later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;pck &amp;lt;- c(&amp;quot;pryr&amp;quot;,&amp;quot;microbenchmark&amp;quot;, &amp;quot;profvis&amp;quot;, &amp;quot;compiler&amp;quot;, &amp;quot;mnormt&amp;quot;)
invisible(lapply(pck, library, character.only = T))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, Turn off the just-in-time compiler. Note that return value is what the JIT was set at previously (default = 3).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;enableJIT(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;the-microbenchmark-function&#34;&gt;The microbenchmark function&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;for evaluating small snippets of code&lt;/li&gt;
&lt;li&gt;below is a comparison of several approaches to calculating a mean&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;a &amp;lt;- function() {
  m &amp;lt;- sample(1:100, 2)
  data.x &amp;lt;- lapply(m, function(x) rnorm(1e4, mean = x))
  do.call(&amp;quot;cbind&amp;quot;, data.x)
}

some_data &amp;lt;- a()
dim(some_data)

microbenchmark(
  mean_loop = apply(some_data, 2, mean),
  mean_vec = colMeans(some_data),
  mean_manual = apply(some_data, 2, function(x) sum(x)/length(x)),
  mean_manual_ultra = apply(some_data, 2, function(x){
    total = 0
    n = 0
    i = 1
    while(!is.na(x[i])) {
      total = total + x[i]
      n = n+1
      i = i+1
    }
    total/n
  })
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;prevent-multiple-dispatch&#34;&gt;Prevent multiple dispatch:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;the function mean() is meant to handle several different types of data&lt;/li&gt;
&lt;li&gt;specifying the method (thus implying a certain type of input) can speed up the process for small data sets&lt;/li&gt;
&lt;li&gt;the function mean() calls a different function depending on the object specified:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;methods(mean)

x1 &amp;lt;- list(e2 = runif(1e2), e4 = runif(1e4), e6 = runif(1e6))

lapply(x1, function(x)
  microbenchmark(
    mean(x),
    mean.default(x)
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I suspect the improvement in speed for smaller objects but larger objects is related to big O notation &amp;ndash; these smaller objects are impacted by constants&lt;/p&gt;
&lt;p&gt;Other approaches for finding source code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# tracking package type, etc
otype(mean)
ftype(mean)
showMethods(mean) #for S4
methods(mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function methods() does not always work, but there are alternatives:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;methods(var)
getAnywhere(var)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;find-the-bottlenecks-with-rprof&#34;&gt;Find the bottlenecks with Rprof()&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;writes stack calls to disk along with memory usage and vector duplication&lt;/li&gt;
&lt;li&gt;you create a .prof file to do this and then close it when done with profiling&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;Rprof(&amp;quot;permute.prof&amp;quot;, memory.profiling = T)

sigma.mv &amp;lt;- diag(1, nrow = 5, ncol = 5)
sigma.mv[upper.tri(sigma.mv)] = 0.5
sigma.mv[lower.tri(sigma.mv)] = 0.5

mvn.data &amp;lt;- rmnorm(1e3, mean = rep(0, 5), varcov = sigma.mv)
colnames(mvn.data) &amp;lt;- c(paste0(&amp;quot;x&amp;quot;,1:5))

kmeans.Ftest &amp;lt;- function(kmean_obj) {
  df.1 = length(kmean_obj$size) - 1
  df.2 = length(kmean_obj$cluster) - length(kmean_obj$size)
  betw_ms &amp;lt;- kmean_obj$tot.withinss/df.1
  with_ms &amp;lt;- kmean_obj$betweenss/df.2
  fratio = betw_ms/with_ms
  pval &amp;lt;- pf(fratio, df1 = df.2, df2 = df.1, lower.tail = F)
  stuff = c(fratio, df.1, df.2, pval)
  names(stuff) &amp;lt;- c(&#39;F-ratio&#39;, &#39;df 1&#39;,&#39;df 2&#39;, &#39;p-value&#39;)
  return(stuff)
}

kmeans.optimiz &amp;lt;- lapply(2:10, function(x) {
    results = kmeans(mvn.data, centers = x, nstart = 15, algorithm = &amp;quot;MacQueen&amp;quot;,
                        iter.max = 50)
    kmeans.Ftest(results)
})

kmeans.final &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, kmeans.optimiz)

Rprof(NULL)
summaryRprof(&amp;quot;permute.prof&amp;quot;) #, memory = &amp;quot;both&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;use-profvis-to-visualize-performance&#34;&gt;Use profvis to visualize performance&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;nice graphical output&lt;/li&gt;
&lt;li&gt;native in RStudio (there is extensive 
&lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-with-RStudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;
)&lt;/li&gt;
&lt;li&gt;you can view output in browser&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;p &amp;lt;- profvis({
mean_loop = apply(some_data, 2, mean)
mean_vec = colMeans(some_data)
mean_manual = apply(some_data, 2, function(x) sum(x)/length(x))
mean_manual_ultra = apply(some_data, 2, function(x){
  total = 0
  n = 0
  i = 1
  while(!is.na(x[i])) {
      total = total + x[i]
      n = n+1
      i = i+1
    }
    total/n
  })
})
htmlwidgets::saveWidget(p, &amp;quot;profile.html&amp;quot;)

browseURL(&amp;quot;profile.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;explore-source-code&#34;&gt;Explore source code&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;How to access function source code&lt;/strong&gt;
(if you didn&#39;t write the function yourself)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Type the function name (without parentheses): &lt;code&gt;eigen&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Find namespace and methods associated: &lt;code&gt;methods(&amp;quot;princomp&amp;quot;); getAnywhere(&amp;quot;princomp.default&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use pryr::show_c_source() to search for C code on GitHub (which may or may not be helpful)&lt;/li&gt;
&lt;li&gt;Download the entire package and explore the code&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;svd
La.svd

try(show_c_source(.Internal(La_svd(x))))
show_c_source(.Internal(mean(x)))

download.packages(&amp;quot;broman&amp;quot;, repos = &amp;quot;https://cloud.r-project.org&amp;quot;,
  destdir = getwd(),type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;trace-objects-copied&#34;&gt;Trace objects copied&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;use tracemem() to track particular objects&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;a &amp;lt;- letters[sample(10)]
tracemem(a)
a[1] &amp;lt;- &amp;quot;Z&amp;quot;
b &amp;lt;- a[1:5]  # not copied
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;memory&#34;&gt;Memory&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;How much memory is being used? Note that R requests memory from your computer in big chunks then manages it itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;mem_used()
object.size(x1) #base
object_size(x1) #pryr
compare_size(x1) #between base R and pryr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read the 
&lt;a href=&#34;https://www.rdocumentation.org/packages/pryr/versions/0.1.4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation for pryr functions&lt;/a&gt;
 for more useful functions.&lt;/p&gt;
&lt;p&gt;Now that we are all done, turn the JIT compiler back on:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;enableJIT(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;more-tips-from-advanced-r&#34;&gt;More Tips from &lt;em&gt;Advanced R&lt;/em&gt;&lt;/h5&gt;
&lt;p&gt;These are designed to reduce internal checks&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;read.csv(): specify known column types with colClasses.&lt;/li&gt;
&lt;li&gt;factor(): specify known levels with levels.&lt;/li&gt;
&lt;li&gt;cut(): don&#39;t generate labels with labels = FALSE if you don&#39;t need them, or, even better, use findInterval().&lt;/li&gt;
&lt;li&gt;unlist(x, use.names = FALSE) is much faster than unlist(x).&lt;/li&gt;
&lt;li&gt;interaction(): if you only need combinations that exist in the data, use drop = TRUE.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;remember&#34;&gt;Remember!&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./xkcd1205.png&#34; alt=&#34;xkcd1205&#34;&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://xkcd.com/1205/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xckd comic&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Practices for Data Workflows in Ag Science</title>
      <link>/post/workflow_basics/</link>
      <pubDate>Tue, 07 Aug 2018 10:50:02 -0700</pubDate>
      <guid>/post/workflow_basics/</guid>
      <description>&lt;p&gt;I was struggling with the data set. The first 50 rows were summary stats (&amp;hellip;I think), and then the actual data started. The file was over 250 columns wide, composed largely of phenotypic traits. Perhaps one third of the columns were unique variables gathered over several years, but, the variable names were inconsistent across years. Actually, there was no information on the variables, what they were measuring, and what scale they were on. The closest hint was a pattern of color coding applied to groups of columns: yellow, green, blue. There appeared to be partial duplication and/or combination of of some columns, but we could not reconstruct exactly what had happened to the data. No one could recall the full history of the file, what that color coding meant, who had pre-processed data, or what they had done. We did know that the raw data were long lost.&lt;/p&gt;
&lt;p&gt;This was the not the first nor the last messy data set with an unknown history that complicated my capacity to organize and analyse the information.&lt;/p&gt;
&lt;p&gt;Other problems I encountered with poorly managed data sets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date sets with outliers removed based on individual&#39;s researchers assessment on what looked weird or not.&lt;/li&gt;
&lt;li&gt;An Excel file several thousand lines with miscellaneous comments added every so often indicating a data point may be suspect.&lt;/li&gt;
&lt;li&gt;An Excel file several sheets wide of the same information across different years - yet the column names did not match, and some sheets included summary stats for groups of data, using empty rows to differentiate groups (again, I think this was what going on&amp;hellip;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There&#39;s no way to sugarcoat it: management of raw experimental data and the analysis workflow is largely abysmal in the agricultural sciences, my academic home for 14 years. The process of cleaning, rearranging, filtering and preparing experimental data for downstream analyses is poorly described, if at all, in most papers. As many of know, much of data conditioning and preparation is done by students and technicians with little formal guidance or documentation of their work.&lt;/p&gt;
&lt;p&gt;Since data sets can and should live on for decades - as part of long-term agriculture research, for meta-analyses or other uses, understanding how the raw data were parsed, cleaned and interpreted is important. Assumptions that made sense in the initial analysis may need re-evaluation later. Intermediate files can turn out to be useful. Organization of an analysis workflow can help researchers conduct their research in a regimented and reproducible way.&lt;/p&gt;
&lt;p&gt;Here are a few concrete recommendations for improving workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Archive the raw, unaltered version of data set.&lt;/li&gt;
&lt;li&gt;Create a consistent directory structure for each project - that way you are storing things consistently and can find them. Really, this makes things easier for everyone.&lt;/li&gt;
&lt;li&gt;For a data set, each column should be include only type of data corresponding to the column name. These data can be on different scales. For example, if one column indicates the type of variable (e.g. pH, moisture content, nitrate concentration), another columns could indicate the actual value for that trait and observation. It is less confusing if a column only contain a similar type of data (e.g &lt;em&gt;raw&lt;/em&gt; OR &lt;em&gt;transformed&lt;/em&gt; OR &lt;em&gt;mean value&lt;/em&gt;), not data and summary stats together.&lt;/li&gt;
&lt;li&gt;Avoid use of formatting that cannot be interpreted as plain text. By this, I mean do not insert comments in Microsoft office files, or use text formatting tools (bolding, color coding) to differentiate observations. This information can easily become lost to time and does not translate well to other software, especially command-line software. Comments, categorical variables, et cetera should each have their own column.&lt;/li&gt;
&lt;li&gt;Summary stats of a data set belong in their own table or file.&lt;/li&gt;
&lt;li&gt;Use a non-proprietary format like CSV or TXT.&lt;/li&gt;
&lt;li&gt;Avoid creating data that relies on a particular order of the observations (for instance, every 5th row is the mean of the previous 4 rows). One sorting step can ruin this. If a special order is needed, create a column which implements this.&lt;/li&gt;
&lt;li&gt;Make a &amp;ldquo;codebook&amp;rdquo; documenting what each variable means, what scale it is on, expected levels or range of data. This can be one sheet of information - very brief, but handy. Giving variables short, descriptive names is also helpful.&lt;/li&gt;
&lt;li&gt;Document what happens to a data set - and make that information available to others. I prefer to use a command line tool like R, but a text file could do. This is to ensure your work can be duplicated. I realize programming is not for everyone, but using a command-line program like R, Python, Julia, SAS, or even bash, is one of the best ways to document how information is pre-processed. Changes made through point-and-click interfaces like Excel are not reproducible because they are almost always incompletely documented or not documented at all. In addition, manual manipulation in a spreadsheet program also results in user-generated errors: misspellings, misclassifications, cut-and-paste errors and so on.&lt;/li&gt;
&lt;li&gt;Accountability and follow up: attach people&#39;s names to their data and/or analysis.&lt;/li&gt;
&lt;li&gt;Back up your data: across at least 2 locations over the life of your project. Version control make it possible for your work to be undone by accessing earlier versions. Git is one such tool, but saving intermediate file versions (often!) can also help. By providing descriptive comments at each new commit (a version), that can help downstream researchers sift through previous versions of the data. Also, some cloud storage providers (like Dropbox) offer options for data recovery.&lt;/li&gt;
&lt;li&gt;Coordinate with project collaborators regarding workflow expectations and processes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The core reason I am interested in these issues is to improve reproducibility. As I dealt with the aforementioned issues with data, I could not help but wonder if I was making the right choices with the data, and wish that I had more information on the background of these legacy data sets. This is particularly important as meta-studies continue to gain in popularity. Unifying data sets and leveraging their combined power is one of the priorities for agricultural productivity identified by the National Academy of Sciences in their recent report, 
&lt;a href=&#34;https://www.nap.edu/resource/25059/ScienceBreakthroughs2030ReportBrief.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Science Breakthroughs to Advance Food and Agricultural Research by 2030&lt;/em&gt;&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;For more information, see this 
&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;excellent article&lt;/em&gt;&lt;/a&gt;
 by Kara Woo and Karl Broman - it&#39;s short and gives easy-to-follow best practices when working with spreadsheets.&lt;/p&gt;
&lt;p&gt;Scott Long at the University of Indiana put together an 
&lt;a href=&#34;https://www.ihrp.uic.edu/files/Workflow%20Slides%20JSLong%20110410.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;informative set of slides&lt;/a&gt;
 delving further into the principles of data workflow. Patrick Schloss, of the bioinformatics program &lt;em&gt;mothur&lt;/em&gt;, recently published an 
&lt;a href=&#34;https://mbio.asm.org/content/9/3/e00525-18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt;
 delving into the problem of reproducibility in microbiome work and how to fix it. Among many things, he recommended implementing robust workflow practices.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Set Up a Virtual Machine for Data Analysis If You Are an IT Admin Noob</title>
      <link>/post/vm_wsu/</link>
      <pubDate>Sun, 18 Mar 2018 10:50:02 -0700</pubDate>
      <guid>/post/vm_wsu/</guid>
      <description>&lt;h2 id=&#34;basic-info&#34;&gt;Basic Info&lt;/h2&gt;
&lt;p&gt;I came back from PAG (Plant &amp;amp; Animal Genome - a huge ag genetics conference held in San Diego each January) all fired about setting up a virtual machine (VM) to facilitate collaboration. The hope what this would enable us to share files and scripts better (without the aid of countless dropboxes everyone forgets about), and improve overall collaboration.&lt;/p&gt;
&lt;p&gt;I had considered this before - enterprise deployment of RStudio server or Jupyter hub, but our program doesn’t have dedicated IT support to do this, and frankly, it all seemed rather intimidating to a plant geneticist like me. But this would be useful. I had once run my laptop on dual boot with Ubuntu and Windows. I could handle this, couldn’t I? All I want is a VM that everyone can access via remote desktop for their analytical needs. Fortunately, I had no idea how difficult this would be, or I might not have entered the lion’s den. So, here are fruits of my labor in the hopes it will ease the process for future users. This guide was developed specific for Washington State University&#39;s College of Agriculture, Human and Natural Resource Sciences (CAHNRS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt;  Request a virtual machine. I used the 
&lt;a href=&#34;https://it.cahnrs.wsu.edu/service-catalog/vm-hosting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;CAHNRS webform&lt;/strong&gt;&lt;/a&gt;
. I went with CentOS 7 - a Linux distribution that had been recommended to me as “full featured” (this remains to be seen). This step may take a few weeks.&lt;/p&gt;
&lt;p&gt;It helps to know exactly what you want to do so CAHNRS IT can properly set up the VM and “rules” (exceptions to their standing rules) to allow you to do the work you want. Running RStudio server means having at least one port open. The RStudio Server default port is 8787 (although than can be easily changed). Remote desktop needs port 3350 open. Jupyter Hub will have its own conventions - all described in their documentation. I recommend you consult the documentation for what you want &lt;em&gt;before&lt;/em&gt; ordering a VM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Download the 
&lt;a href=&#34;https://its.wsu.edu/ssl-vpn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;WSU VPN client&lt;/strong&gt;&lt;/a&gt;
. Whoever is the admin will need have the WSU SSLVPN to tunnel in through a secure connection in order to do any work remotely. This is the probably the easiest step of the entire process. Who doesn’t love point-and-click installations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Download software for SSH. Initially, you will need an SSH program to connect through secure, encrypted tunnel. 
&lt;a href=&#34;https://www.chiark.greenend.org.uk/~sgtatham/putty/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Putty&lt;/strong&gt;&lt;/a&gt;
 is a popular option. I like secure shell, which I found 
&lt;a href=&#34;https://www.wm.edu/offices/it/services/software/licensedsoftware/webeditingsftp/sshsecureshell/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;
. This is actually a rather convenient way to manage the VM, so its utility will likely stretch past the setup phase.&lt;/p&gt;
&lt;h2 id=&#34;general-linux-notes&#34;&gt;General Linux notes:&lt;/h2&gt;
&lt;p&gt;This is CentOS, an alternative distribution to its more popular relative, Ubuntu. While most commands are the same across Linux distributions, some things are a little different. FYI, Centos is similar to .rhel and Red Hat if you are combing forums for help.&lt;/p&gt;
&lt;p&gt;Check out these 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;standards for Linux filesystems&lt;/strong&gt;&lt;/a&gt;
. It will help you understand how the machine is organized. Don’t spend too much time here; it’s more of a reference than pleasure reading.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.centos.org/docs/5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Centos documentation&lt;/strong&gt;&lt;/a&gt;
 can be helpful, as well.&lt;/p&gt;
&lt;h3 id=&#34;miscellaneous-useful-commands&#34;&gt;Miscellaneous useful commands:&lt;/h3&gt;
&lt;p&gt;Most of the command listed here for VM setup require root privileges, which can be accomplished with “sudo”. You’ll find you need to be root 98% of time during VM setup, so do this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo su&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This changes the console from:
&lt;code&gt;$ &lt;/code&gt;
to:
&lt;code&gt;#   &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Use Ctrl-D to leave root. And be careful when you are root!&lt;/p&gt;
&lt;p&gt;Print current working directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pwd&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Change the working directory to go up a level:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd .. &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Absolute path (will work everywhere):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$ cd /&amp;lt;path&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Relative path (only sees child directories):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This produces a long list of all files and directories in your current working directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ll -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To see options associated with a particular command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;command&amp;gt; --help&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To remove a single file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rm &amp;lt;your_file.txt&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To remove an entire directory:&lt;br&gt;
‘r’ does it recursively&lt;br&gt;
‘f’ will do it without asking again (it skips the step that is essentially asking “are you sure you want to delete this????”)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rm -rf &amp;lt;your_directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To install a piece of software for all users:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo yum install &amp;lt;library&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To view files:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;more &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To edit files. This command requires sudo for most of the files you will need to look at for VM setup.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nano &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There’s also the vim editor, but use it your own risk. Here is the best description I have ever read of it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If you want an outstanding free text editor and don&#39;t mind a seemingly vertical learning curve plus long days of pain and suffering while all your neural pathways are rewired, try Vim.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;vi &amp;lt;file&amp;gt; &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To escape vim:&lt;/p&gt;
&lt;p&gt;&lt;code&gt; :q &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Make a new directory:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mkdir &amp;lt;dir_name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Setting your VM to the correct timezone (Pacific in this case). It is helpful to have the correct time when you are inspecting the logs during troubleshooting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;timedatectl set-timezone &#39;America/Los_Angeles&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;managing-users&#34;&gt;Managing Users&lt;/h3&gt;
&lt;p&gt;It is important to think thru how to organize the users into groups and create folders with shared permissions in order to enable collaboration.&lt;/p&gt;
&lt;p&gt;Below are some common commands. Note that they all require root or sudo access. Centos provides excellent 
&lt;a href=&#34;https://www.centos.org/docs/5/html/Deployment_Guide-en-US/s1-users-tools.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;
 on this.&lt;/p&gt;
&lt;p&gt;Add a new user (the second option adds a new user to an existing group):&lt;br&gt;
&lt;code&gt;useradd &amp;lt;username&amp;gt;&lt;/code&gt;&lt;br&gt;
&lt;code&gt;useradd &amp;lt;username&amp;gt; -g &amp;lt;groupname&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Set user password:&lt;br&gt;
&lt;code&gt;passwd &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Add a group:&lt;br&gt;
&lt;code&gt;groupadd &amp;lt;groupname&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Add an existing user to an existing group:&lt;br&gt;
&lt;code&gt;sudo usermod -a -G &amp;lt;groupname&amp;gt; &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Enable file ownership by a group:&lt;br&gt;
&lt;code&gt;chown :&amp;lt;groupname&amp;gt; &amp;lt;path/to/directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Enable file ownership by a user:&lt;br&gt;
&lt;code&gt;chown :&amp;lt;username&amp;gt; &amp;lt;path/to/directory&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Delete a user:&lt;br&gt;
&lt;code&gt;userdel &amp;lt;username&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Link directories:&lt;br&gt;
&lt;code&gt;ln -s &amp;lt;path/to/file&amp;gt; &amp;lt;path/to/link&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;if-you-want-to-run-remote-desktop&#34;&gt;If you want to run remote desktop:&lt;/h2&gt;
&lt;p&gt;There are other desktop environments, but Gnome is a popular one, so why not?&lt;/p&gt;
&lt;p&gt;See what is available:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum group list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install Gnome desktop environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install gnome
sudo yum install gnome desktop environment
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check that it is running&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ps -aux | grep gnome
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install and configure the EPEL repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the nux repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install xrdp:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum -y install xrdp tigervnc*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll need to start xrdp and xrdp-sesman (“session manager”) as a service (so they are always running) and enable them both to autostart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl start xrdp
sudo systemctl enable xrdp
sudo systemctl start xrdp-sesman
sudo systemctl enable xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And check:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl status xrdp
sudo systemctl status xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These should indicate that xrdp and xrdp-sesman are &lt;em&gt;active&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Open a port to allow remote desktop connections:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo firewall-cmd --permanent --zone=public --add-port=3389/tcp
sudo firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Configure SELinux:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo chcon --type=bin_t /usr/sbin/xrdp
sudo chcon --type=bin_t /usr/sbin/xrdp-sesman
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And check that the port is open:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;netstat -plan | grep xrdp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, vnc has to be started. It is tied to a user, so get out of root if you are there (and don’t use sudo).&lt;/p&gt;
&lt;p&gt;Set a password:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vncpasswd -&amp;lt;user&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It will return a prompt, asking you for a password. This is what you will use for remote desktop. You can definitely skip the  step of having a read-only password by answering “n” for no. (It’s unclear to me why anyone would want a password for that as opposed to whole new user account with read-only permissions, but what do I know?).&lt;/p&gt;
&lt;p&gt;Next, start the server. Try to only do this once.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vnc server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you end up with multiple vnc server instances running on just ONE user account, kill the extra processes and delete their log files. Here are the steps:&lt;/p&gt;
&lt;p&gt;First, find the process ID’s:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ps aux | grep vnc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The process ID&#39;s (pid) are located in the second column from the left. Kill the extra vnc pid&#39;s:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kill -9 &amp;lt;pid&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Find the associated files and delete them. I found there here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /var/log
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They will have names like &amp;ldquo;Xorg.9.log&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;troubleshooting-remote-desktop&#34;&gt;Troubleshooting Remote Desktop&lt;/h3&gt;
&lt;p&gt;If you run into trouble, try disabling SELinux:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo setenforce 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;getenforce
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(it should say “permissive” NOT “enforcing)&lt;/p&gt;
&lt;p&gt;Check for problems (as root):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /var/logs
more xrdp.log
more xrdp-sesman.log
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are cleared rapidly and folded into the xrdp .tar files, so do not be alarmed if the log files are empty. If you really want to see something in the xrdp logs, stop and restart xrdp.&lt;/p&gt;
&lt;p&gt;The most useful file is the messages file, but it’s very long, so just show the end:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo tail -55 messages
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you &lt;em&gt;really&lt;/em&gt; want to see what&#39;s going on, watch the file in real time:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tail -f messages&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Ctlr-C to leave&lt;/p&gt;
&lt;p&gt;The firewalld file in the /var/log/ directory is stuffed with known, but currently unresolved bugs so it’s not very useful for diagnosing problems at this time.&lt;/p&gt;
&lt;p&gt;There&#39;s a number of remote desktop applications which can be used to access the VM. Windows RDP is easy to use and secure.&lt;/p&gt;
&lt;h3 id=&#34;run-r-in-remote-desktop&#34;&gt;Run R in remote desktop&lt;/h3&gt;
&lt;p&gt;Start by installing R. WSU would not let me access the internet directly from the remote desktop internet browser, but I could through the SSH. So here are command line instructions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install epel-release
sudo yum update
sudo yum install R -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, install RStudio. Get current link from the 
&lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio website&lt;/strong&gt;&lt;/a&gt;
. The link below worked March 19, 2018:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://download1.rstudio.org/rstudio-1.1.442-x86_6
sudo rpm -Uvh rstudio-1.1.442-x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other&#34;&gt;Other&lt;/h2&gt;
&lt;p&gt;It&#39;s also helpful to have an office suite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install libreoffice
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, all done! Go and solve the world’s problems!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What I learned at the UseR! conference</title>
      <link>/post/user_2018/</link>
      <pubDate>Sun, 18 Mar 2018 10:50:02 -0700</pubDate>
      <guid>/post/user_2018/</guid>
      <description>&lt;h4 id=&#34;top-lessons-from-r-users-conference-held-july-10---13-in-brisbane-aus&#34;&gt;Top Lessons from R users conference held July 10 - 13 in Brisbane, AUS&lt;/h4&gt;
&lt;p&gt;With 900 registrants and dozens of talks, there is much to report (
&lt;a href=&#34;https://www.youtube.com/channel/UC_R5smHVXRYGhZYDJsnXTwg/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;videos of most talks&lt;/a&gt;
 provided by R Consortium YouTube channel) I&#39;ll skip loads of it and just focus on the top 10 cool stuff.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The hex wall was just straight up cool. Here&#39;s the 
&lt;a href=&#34;https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code for that&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I participated in a half day workshop on Rcpp (
&lt;a href=&#34;https://www.youtube.com/watch?v=FZ0LcJbxPF0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 and 
&lt;a href=&#34;https://www.youtube.com/watch?v=EXGhR-kyjRg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
). This continues to be a very popular suite of packages to help users increase the speed and efficiency of their programs. I say &amp;ldquo;suite&amp;rdquo; because in addition to 3 major functions provided by Rcpp, a number of accessory packages have been written to extend its functionality. Several of these accessory packages essentially are providing templates for invoking C++ commands without having to actually know C. Dirk Eddelbuttel introduced the 3 major functions for extending R with C/C++:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;evalCpp: for evaluating ad hoc anonymous expressions&lt;/li&gt;
&lt;li&gt;cppFunction: standard function written in C/C++ and invoked in a program&lt;/li&gt;
&lt;li&gt;sourceCpp: source C/C++ objects &amp;amp; functions from external file
These functions must be compiled each time a fresh R session is started. Building an Rcpp package avoids this and is easy to do:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Rcpp.package.skeleton(&amp;quot;mypackage&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;The other useful workshop I participated in was methods for speeding up R, delivered by Thomas Lumley, formerly of UW and now of U of Auckland. He focused on a few major recommendations:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vectorise whenever possible. This also implies that you know your vectorised functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lapply/apply/sapply/vapply/tapply&lt;/li&gt;
&lt;li&gt;rowMeans, rowSums&lt;/li&gt;
&lt;li&gt;colMeans, colSums&lt;/li&gt;
&lt;li&gt;Consider parallelised functions: mclapply&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Base functions are often designed to handle wildly different input. It may make sense in some instances to rewrite functions, making assumptions (e.g. about the input data) that fit your circumstances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data frames should be avoided if possible - they are expensive to create and often copied in whole when modified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using an optimized matrix algebra library (not LAPACK) may be worth the time to install that. These libraries have optimized how data is accessed from disk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some rec&#39;s on how to handle large objects that reasonably don&#39;t fit in memory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;buy a computer with more memory (not always a reasonable option)&lt;/li&gt;
&lt;li&gt;put your data in database and access with tools like dbplyr. One recommendation is MonetDBLite, a column-optimized database good for scientific applications for that reason.&lt;/li&gt;
&lt;li&gt;Use special file formats for bigger data sets: HDF5, GitLFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But, first and foremost, he emphasized the use of profiling tools and benchmarking functions. find out where the bottleneck in your program actually are! And be careful to not waste too much time on optimizing code - time saved may not be worth time invested. R is automatically running a JIT (just in time) compiler that makes code run faster each time it is called. It makes sense to turn this off while doing benchmarking:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;enableJIT(0)

# to turn back on:

enableJIT(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Course notes available on his github repo 
&lt;a href=&#34;https://github.com/tslumley/useRfasteR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UseRfasteR&lt;/a&gt;
.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent keynotes all-around. 
&lt;a href=&#34;https://www.youtube.com/watch?v=27FxbDtCFoc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steph da Silva&lt;/a&gt;
 emphasized the importance of &lt;em&gt;community&lt;/em&gt; in open-source community - what that can look like, how to contribute  how your contribution helps develop camaraderie to support sharing of code and development of analysts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=5033jBHFiHE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Peng discussed development of R&lt;/a&gt;
 as it strives to fit both programmers and regular ole scientists analyzing their data. He summarized his own talk quite well in this essay. He described the rise of &amp;ldquo;worse is better&amp;rdquo;, that is, the simplying of options in R to make it easier for new users to learn. The tidyverse is considered such an example. However, as a long-time user, I find dplyr&#39;s group-and-summarise options (among others) to be incredibly handy. And I will never go back to using reshape() now that we have gather() and spread(). Even reshape2() is a massive improvement in usability!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jenny Bryan&#39;s talk on 
&lt;a href=&#34;https://youtu.be/7oyiPBjLAWY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Code Smells and Feels&amp;rdquo;&lt;/a&gt;
 provided a great introduction to how to identify and fix poorly written code. According to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Code_smell&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
, &amp;ldquo;Code smells are usually not bugs; they are not technically incorrect and do not prevent the program from functioning. Instead, they indicate weaknesses in design that may slow down development or increase the risk of bugs or failures in the future.&amp;rdquo;
Based on the book, &lt;em&gt;Refactoring: Improving the Design of Existing Code&lt;/em&gt;, she gave a few simple directives to create simple code that is easy to understand, debug, and maintain. Hooray!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Hoist entry or exit conditionals to the top of functions&lt;/li&gt;
&lt;li&gt;Use functions as much as possible&lt;/li&gt;
&lt;li&gt;Avoid overly nested code (e.g. a long cascade of nested &amp;ldquo;if&amp;rdquo; statements)&lt;/li&gt;
&lt;li&gt;Not every &amp;ldquo;if&amp;rdquo; needs an else&lt;/li&gt;
&lt;li&gt;Consider implementing true object-oriented programming when working with classes&lt;/li&gt;
&lt;li&gt;Don&#39;t&amp;rsquo; be afraid to write short helper functions
There was also a brief discussion on how to tell a person their code smells bad (&amp;ldquo;that&#39;s what we have funny names like &amp;lsquo;excessive use of literals&amp;rsquo;&amp;quot;).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excellent set of talks on data handling and workflow. In particular, 
&lt;a href=&#34;https://www.youtube.com/watch?v=GrqM2VqIQ20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a talk on the website builder&lt;/a&gt;
 for documenting data analysis called 
&lt;a href=&#34;https://github.com/jdblischak/workflowr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;workflowr&amp;rdquo;&lt;/a&gt;
. 
&lt;a href=&#34;https://www.youtube.com/watch?v=IYfZ6kd7aT0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discussion on consistent workflows&lt;/a&gt;
, recommended conventions for naming variables, creating consistent set of directories for each project, using version controls. Obvious to a data scientist, but rarely used by regular scientist. This resonates strongly with me due to ongoing issues with data management in the agricultural sciences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=3Bu7QUxzIbA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Excellent talk on precision in R&lt;/a&gt;
. Use .Machine to find out more, but basically R is only good to 16 decimal points. Subtraction of one number similar to it can result in cancellation where the result essentially slides to zero, losing precision. Ways to avoid this include working in log space, perhaps adding a zero if needed: log(x + 1), -log(1 - exp(x)). Also, there is the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/Rmpfr/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmpfr package&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=XQmBcpQl8K8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glue package&lt;/a&gt;
 - for &amp;ldquo;gluing&amp;rdquo; strings to data, like string interpolation in bash (&amp;ldquo;$&amp;rdquo;) or python f&amp;rdquo;{&amp;hellip;}&amp;quot;. R does have sprintf() with with identical functionality to C&#39;s printf(), but glue makes this easier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=L6FawdEA3W0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fun talk on vwline package&lt;/a&gt;
 for creating variable-width lines, like in Menaurd&#39;s famous plot of Napolean&#39;s march through Russia.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;./napoleon.jpg&#34; alt=&#34;napolean_plot&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
